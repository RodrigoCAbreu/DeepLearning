{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando Redes Neurais com Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializadores em Keras: https://keras.io/initializers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de Ativação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/activations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron - Versão 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rede neural com apenas uma camada oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print (tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "11501568/11490434 [==============================] - 0s 0us/step\n",
      "60000 exemplos de treino\n",
      "10000 exemplos de teste\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "375/375 [==============================] - 2s 2ms/step - loss: 1.4115 - accuracy: 0.6351 - val_loss: 0.9079 - val_accuracy: 0.8231\n",
      "Epoch 2/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.8033 - accuracy: 0.8240 - val_loss: 0.6632 - val_accuracy: 0.8557\n",
      "Epoch 3/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.6486 - accuracy: 0.8464 - val_loss: 0.5653 - val_accuracy: 0.8676\n",
      "Epoch 4/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5742 - accuracy: 0.8584 - val_loss: 0.5112 - val_accuracy: 0.8755\n",
      "Epoch 5/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5291 - accuracy: 0.8665 - val_loss: 0.4764 - val_accuracy: 0.8805\n",
      "Epoch 6/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4983 - accuracy: 0.8721 - val_loss: 0.4519 - val_accuracy: 0.8858\n",
      "Epoch 7/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4755 - accuracy: 0.8762 - val_loss: 0.4335 - val_accuracy: 0.8897\n",
      "Epoch 8/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4579 - accuracy: 0.8789 - val_loss: 0.4190 - val_accuracy: 0.8919\n",
      "Epoch 9/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4437 - accuracy: 0.8821 - val_loss: 0.4072 - val_accuracy: 0.8957\n",
      "Epoch 10/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4320 - accuracy: 0.8846 - val_loss: 0.3977 - val_accuracy: 0.8960\n",
      "Epoch 11/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4221 - accuracy: 0.8864 - val_loss: 0.3894 - val_accuracy: 0.8986\n",
      "Epoch 12/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4136 - accuracy: 0.8883 - val_loss: 0.3825 - val_accuracy: 0.8987\n",
      "Epoch 13/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4062 - accuracy: 0.8900 - val_loss: 0.3762 - val_accuracy: 0.9006\n",
      "Epoch 14/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3997 - accuracy: 0.8913 - val_loss: 0.3708 - val_accuracy: 0.9009\n",
      "Epoch 15/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3938 - accuracy: 0.8925 - val_loss: 0.3661 - val_accuracy: 0.9020\n",
      "Epoch 16/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3886 - accuracy: 0.8938 - val_loss: 0.3618 - val_accuracy: 0.9030\n",
      "Epoch 17/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3839 - accuracy: 0.8949 - val_loss: 0.3578 - val_accuracy: 0.9043\n",
      "Epoch 18/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3796 - accuracy: 0.8961 - val_loss: 0.3544 - val_accuracy: 0.9044\n",
      "Epoch 19/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3756 - accuracy: 0.8967 - val_loss: 0.3509 - val_accuracy: 0.9048\n",
      "Epoch 20/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3720 - accuracy: 0.8974 - val_loss: 0.3481 - val_accuracy: 0.9062\n",
      "Epoch 21/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3686 - accuracy: 0.8984 - val_loss: 0.3452 - val_accuracy: 0.9072\n",
      "Epoch 22/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3655 - accuracy: 0.8993 - val_loss: 0.3425 - val_accuracy: 0.9069\n",
      "Epoch 23/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3626 - accuracy: 0.9000 - val_loss: 0.3402 - val_accuracy: 0.9072\n",
      "Epoch 24/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3599 - accuracy: 0.9004 - val_loss: 0.3381 - val_accuracy: 0.9081\n",
      "Epoch 25/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3573 - accuracy: 0.9009 - val_loss: 0.3360 - val_accuracy: 0.9082\n",
      "Epoch 26/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3550 - accuracy: 0.9015 - val_loss: 0.3340 - val_accuracy: 0.9088\n",
      "Epoch 27/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3527 - accuracy: 0.9019 - val_loss: 0.3323 - val_accuracy: 0.9092\n",
      "Epoch 28/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3506 - accuracy: 0.9026 - val_loss: 0.3305 - val_accuracy: 0.9101\n",
      "Epoch 29/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3485 - accuracy: 0.9029 - val_loss: 0.3287 - val_accuracy: 0.9109\n",
      "Epoch 30/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3466 - accuracy: 0.9036 - val_loss: 0.3273 - val_accuracy: 0.9106\n",
      "Epoch 31/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3448 - accuracy: 0.9038 - val_loss: 0.3258 - val_accuracy: 0.9112\n",
      "Epoch 32/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3431 - accuracy: 0.9045 - val_loss: 0.3244 - val_accuracy: 0.9114\n",
      "Epoch 33/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3414 - accuracy: 0.9049 - val_loss: 0.3231 - val_accuracy: 0.9123\n",
      "Epoch 34/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3398 - accuracy: 0.9054 - val_loss: 0.3218 - val_accuracy: 0.9128\n",
      "Epoch 35/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3383 - accuracy: 0.9054 - val_loss: 0.3205 - val_accuracy: 0.9126\n",
      "Epoch 36/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3368 - accuracy: 0.9061 - val_loss: 0.3194 - val_accuracy: 0.9129\n",
      "Epoch 37/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3355 - accuracy: 0.9066 - val_loss: 0.3183 - val_accuracy: 0.9135\n",
      "Epoch 38/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3341 - accuracy: 0.9065 - val_loss: 0.3172 - val_accuracy: 0.9139\n",
      "Epoch 39/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3328 - accuracy: 0.9072 - val_loss: 0.3162 - val_accuracy: 0.9142\n",
      "Epoch 40/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3316 - accuracy: 0.9075 - val_loss: 0.3152 - val_accuracy: 0.9143\n",
      "Epoch 41/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3304 - accuracy: 0.9076 - val_loss: 0.3142 - val_accuracy: 0.9138\n",
      "Epoch 42/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3292 - accuracy: 0.9080 - val_loss: 0.3134 - val_accuracy: 0.9141\n",
      "Epoch 43/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3281 - accuracy: 0.9082 - val_loss: 0.3124 - val_accuracy: 0.9148\n",
      "Epoch 44/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3270 - accuracy: 0.9086 - val_loss: 0.3116 - val_accuracy: 0.9144\n",
      "Epoch 45/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3260 - accuracy: 0.9088 - val_loss: 0.3108 - val_accuracy: 0.9149\n",
      "Epoch 46/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3250 - accuracy: 0.9090 - val_loss: 0.3100 - val_accuracy: 0.9147\n",
      "Epoch 47/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3240 - accuracy: 0.9093 - val_loss: 0.3093 - val_accuracy: 0.9150\n",
      "Epoch 48/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3231 - accuracy: 0.9094 - val_loss: 0.3085 - val_accuracy: 0.9153\n",
      "Epoch 49/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3222 - accuracy: 0.9099 - val_loss: 0.3078 - val_accuracy: 0.9156\n",
      "Epoch 50/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3213 - accuracy: 0.9100 - val_loss: 0.3071 - val_accuracy: 0.9158\n",
      "Epoch 51/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3205 - accuracy: 0.9102 - val_loss: 0.3064 - val_accuracy: 0.9160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3196 - accuracy: 0.9108 - val_loss: 0.3058 - val_accuracy: 0.9158\n",
      "Epoch 53/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3188 - accuracy: 0.9111 - val_loss: 0.3050 - val_accuracy: 0.9158\n",
      "Epoch 54/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3180 - accuracy: 0.9115 - val_loss: 0.3044 - val_accuracy: 0.9164\n",
      "Epoch 55/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3173 - accuracy: 0.9115 - val_loss: 0.3038 - val_accuracy: 0.9162\n",
      "Epoch 56/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3165 - accuracy: 0.9115 - val_loss: 0.3034 - val_accuracy: 0.9161\n",
      "Epoch 57/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3158 - accuracy: 0.9124 - val_loss: 0.3027 - val_accuracy: 0.9162\n",
      "Epoch 58/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3151 - accuracy: 0.9124 - val_loss: 0.3023 - val_accuracy: 0.9159\n",
      "Epoch 59/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3143 - accuracy: 0.9124 - val_loss: 0.3016 - val_accuracy: 0.9166\n",
      "Epoch 60/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3137 - accuracy: 0.9127 - val_loss: 0.3011 - val_accuracy: 0.9168\n",
      "Epoch 61/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3130 - accuracy: 0.9127 - val_loss: 0.3006 - val_accuracy: 0.9172\n",
      "Epoch 62/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3124 - accuracy: 0.9128 - val_loss: 0.3001 - val_accuracy: 0.9179\n",
      "Epoch 63/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3117 - accuracy: 0.9133 - val_loss: 0.2998 - val_accuracy: 0.9168\n",
      "Epoch 64/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3111 - accuracy: 0.9132 - val_loss: 0.2992 - val_accuracy: 0.9177\n",
      "Epoch 65/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3106 - accuracy: 0.9134 - val_loss: 0.2987 - val_accuracy: 0.9173\n",
      "Epoch 66/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3099 - accuracy: 0.9135 - val_loss: 0.2983 - val_accuracy: 0.9178\n",
      "Epoch 67/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3093 - accuracy: 0.9141 - val_loss: 0.2979 - val_accuracy: 0.9175\n",
      "Epoch 68/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3088 - accuracy: 0.9141 - val_loss: 0.2974 - val_accuracy: 0.9182\n",
      "Epoch 69/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3082 - accuracy: 0.9141 - val_loss: 0.2971 - val_accuracy: 0.9173\n",
      "Epoch 70/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3078 - accuracy: 0.9141 - val_loss: 0.2966 - val_accuracy: 0.9178\n",
      "Epoch 71/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3072 - accuracy: 0.9143 - val_loss: 0.2962 - val_accuracy: 0.9181\n",
      "Epoch 72/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3067 - accuracy: 0.9147 - val_loss: 0.2958 - val_accuracy: 0.9180\n",
      "Epoch 73/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3062 - accuracy: 0.9147 - val_loss: 0.2954 - val_accuracy: 0.9180\n",
      "Epoch 74/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3057 - accuracy: 0.9147 - val_loss: 0.2951 - val_accuracy: 0.9187\n",
      "Epoch 75/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3052 - accuracy: 0.9149 - val_loss: 0.2947 - val_accuracy: 0.9186\n",
      "Epoch 76/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3047 - accuracy: 0.9152 - val_loss: 0.2944 - val_accuracy: 0.9184\n",
      "Epoch 77/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3042 - accuracy: 0.9153 - val_loss: 0.2941 - val_accuracy: 0.9176\n",
      "Epoch 78/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3038 - accuracy: 0.9151 - val_loss: 0.2937 - val_accuracy: 0.9186\n",
      "Epoch 79/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3033 - accuracy: 0.9154 - val_loss: 0.2935 - val_accuracy: 0.9182\n",
      "Epoch 80/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3029 - accuracy: 0.9157 - val_loss: 0.2930 - val_accuracy: 0.9190\n",
      "Epoch 81/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3024 - accuracy: 0.9156 - val_loss: 0.2927 - val_accuracy: 0.9190\n",
      "Epoch 82/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3020 - accuracy: 0.9159 - val_loss: 0.2924 - val_accuracy: 0.9189\n",
      "Epoch 83/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3016 - accuracy: 0.9160 - val_loss: 0.2921 - val_accuracy: 0.9186\n",
      "Epoch 84/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3012 - accuracy: 0.9158 - val_loss: 0.2918 - val_accuracy: 0.9189\n",
      "Epoch 85/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3008 - accuracy: 0.9164 - val_loss: 0.2914 - val_accuracy: 0.9188\n",
      "Epoch 86/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3004 - accuracy: 0.9160 - val_loss: 0.2912 - val_accuracy: 0.9192\n",
      "Epoch 87/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3000 - accuracy: 0.9165 - val_loss: 0.2909 - val_accuracy: 0.9194\n",
      "Epoch 88/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2996 - accuracy: 0.9162 - val_loss: 0.2906 - val_accuracy: 0.9192\n",
      "Epoch 89/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2992 - accuracy: 0.9166 - val_loss: 0.2903 - val_accuracy: 0.9194\n",
      "Epoch 90/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2988 - accuracy: 0.9169 - val_loss: 0.2900 - val_accuracy: 0.9193\n",
      "Epoch 91/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2985 - accuracy: 0.9167 - val_loss: 0.2898 - val_accuracy: 0.9191\n",
      "Epoch 92/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2981 - accuracy: 0.9170 - val_loss: 0.2895 - val_accuracy: 0.9193\n",
      "Epoch 93/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2978 - accuracy: 0.9168 - val_loss: 0.2893 - val_accuracy: 0.9194\n",
      "Epoch 94/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2974 - accuracy: 0.9171 - val_loss: 0.2890 - val_accuracy: 0.9197\n",
      "Epoch 95/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2971 - accuracy: 0.9170 - val_loss: 0.2888 - val_accuracy: 0.9193\n",
      "Epoch 96/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2967 - accuracy: 0.9174 - val_loss: 0.2886 - val_accuracy: 0.9197\n",
      "Epoch 97/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2964 - accuracy: 0.9173 - val_loss: 0.2883 - val_accuracy: 0.9197\n",
      "Epoch 98/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2961 - accuracy: 0.9175 - val_loss: 0.2880 - val_accuracy: 0.9197\n",
      "Epoch 99/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2958 - accuracy: 0.9178 - val_loss: 0.2878 - val_accuracy: 0.9197\n",
      "Epoch 100/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2954 - accuracy: 0.9177 - val_loss: 0.2877 - val_accuracy: 0.9197\n",
      "Epoch 101/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2951 - accuracy: 0.9173 - val_loss: 0.2874 - val_accuracy: 0.9200\n",
      "Epoch 102/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2948 - accuracy: 0.9177 - val_loss: 0.2872 - val_accuracy: 0.9196\n",
      "Epoch 103/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2945 - accuracy: 0.9177 - val_loss: 0.2869 - val_accuracy: 0.9200\n",
      "Epoch 104/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2942 - accuracy: 0.9181 - val_loss: 0.2867 - val_accuracy: 0.9199\n",
      "Epoch 105/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2939 - accuracy: 0.9179 - val_loss: 0.2865 - val_accuracy: 0.9199\n",
      "Epoch 106/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2936 - accuracy: 0.9181 - val_loss: 0.2864 - val_accuracy: 0.9200\n",
      "Epoch 107/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2933 - accuracy: 0.9180 - val_loss: 0.2861 - val_accuracy: 0.9202\n",
      "Epoch 108/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2930 - accuracy: 0.9184 - val_loss: 0.2859 - val_accuracy: 0.9203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2927 - accuracy: 0.9182 - val_loss: 0.2857 - val_accuracy: 0.9200\n",
      "Epoch 110/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2924 - accuracy: 0.9184 - val_loss: 0.2856 - val_accuracy: 0.9203\n",
      "Epoch 111/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2922 - accuracy: 0.9183 - val_loss: 0.2853 - val_accuracy: 0.9202\n",
      "Epoch 112/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2919 - accuracy: 0.9184 - val_loss: 0.2851 - val_accuracy: 0.9199\n",
      "Epoch 113/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2916 - accuracy: 0.9188 - val_loss: 0.2849 - val_accuracy: 0.9204\n",
      "Epoch 114/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2914 - accuracy: 0.9185 - val_loss: 0.2848 - val_accuracy: 0.9207\n",
      "Epoch 115/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2911 - accuracy: 0.9187 - val_loss: 0.2846 - val_accuracy: 0.9204\n",
      "Epoch 116/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2909 - accuracy: 0.9189 - val_loss: 0.2844 - val_accuracy: 0.9202\n",
      "Epoch 117/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2906 - accuracy: 0.9192 - val_loss: 0.2842 - val_accuracy: 0.9205\n",
      "Epoch 118/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2903 - accuracy: 0.9191 - val_loss: 0.2840 - val_accuracy: 0.9206\n",
      "Epoch 119/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2901 - accuracy: 0.9191 - val_loss: 0.2839 - val_accuracy: 0.9207\n",
      "Epoch 120/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2898 - accuracy: 0.9192 - val_loss: 0.2837 - val_accuracy: 0.9206\n",
      "Epoch 121/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2896 - accuracy: 0.9192 - val_loss: 0.2835 - val_accuracy: 0.9207\n",
      "Epoch 122/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2893 - accuracy: 0.9194 - val_loss: 0.2834 - val_accuracy: 0.9205\n",
      "Epoch 123/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2891 - accuracy: 0.9193 - val_loss: 0.2832 - val_accuracy: 0.9210\n",
      "Epoch 124/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2889 - accuracy: 0.9193 - val_loss: 0.2831 - val_accuracy: 0.9210\n",
      "Epoch 125/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2886 - accuracy: 0.9196 - val_loss: 0.2829 - val_accuracy: 0.9208\n",
      "Epoch 126/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2884 - accuracy: 0.9195 - val_loss: 0.2828 - val_accuracy: 0.9206\n",
      "Epoch 127/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2881 - accuracy: 0.9200 - val_loss: 0.2827 - val_accuracy: 0.9209\n",
      "Epoch 128/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2880 - accuracy: 0.9198 - val_loss: 0.2824 - val_accuracy: 0.9208\n",
      "Epoch 129/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2877 - accuracy: 0.9198 - val_loss: 0.2823 - val_accuracy: 0.9208\n",
      "Epoch 130/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2875 - accuracy: 0.9197 - val_loss: 0.2821 - val_accuracy: 0.9208\n",
      "Epoch 131/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2873 - accuracy: 0.9199 - val_loss: 0.2821 - val_accuracy: 0.9214\n",
      "Epoch 132/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2871 - accuracy: 0.9201 - val_loss: 0.2818 - val_accuracy: 0.9213\n",
      "Epoch 133/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2869 - accuracy: 0.9199 - val_loss: 0.2817 - val_accuracy: 0.9211\n",
      "Epoch 134/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2867 - accuracy: 0.9200 - val_loss: 0.2816 - val_accuracy: 0.9212\n",
      "Epoch 135/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2864 - accuracy: 0.9200 - val_loss: 0.2814 - val_accuracy: 0.9210\n",
      "Epoch 136/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2862 - accuracy: 0.9199 - val_loss: 0.2814 - val_accuracy: 0.9213\n",
      "Epoch 137/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2860 - accuracy: 0.9201 - val_loss: 0.2811 - val_accuracy: 0.9217\n",
      "Epoch 138/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2858 - accuracy: 0.9203 - val_loss: 0.2810 - val_accuracy: 0.9215\n",
      "Epoch 139/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2856 - accuracy: 0.9202 - val_loss: 0.2809 - val_accuracy: 0.9215\n",
      "Epoch 140/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2854 - accuracy: 0.9204 - val_loss: 0.2809 - val_accuracy: 0.9214\n",
      "Epoch 141/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2852 - accuracy: 0.9204 - val_loss: 0.2807 - val_accuracy: 0.9217\n",
      "Epoch 142/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2850 - accuracy: 0.9205 - val_loss: 0.2805 - val_accuracy: 0.9218\n",
      "Epoch 143/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2848 - accuracy: 0.9206 - val_loss: 0.2804 - val_accuracy: 0.9218\n",
      "Epoch 144/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2846 - accuracy: 0.9204 - val_loss: 0.2803 - val_accuracy: 0.9221\n",
      "Epoch 145/200\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.92 - 1s 2ms/step - loss: 0.2844 - accuracy: 0.9205 - val_loss: 0.2802 - val_accuracy: 0.9218\n",
      "Epoch 146/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2843 - accuracy: 0.9206 - val_loss: 0.2800 - val_accuracy: 0.9224\n",
      "Epoch 147/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2841 - accuracy: 0.9209 - val_loss: 0.2800 - val_accuracy: 0.9221\n",
      "Epoch 148/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2839 - accuracy: 0.9207 - val_loss: 0.2798 - val_accuracy: 0.9223\n",
      "Epoch 149/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2837 - accuracy: 0.9208 - val_loss: 0.2797 - val_accuracy: 0.9222\n",
      "Epoch 150/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2835 - accuracy: 0.9208 - val_loss: 0.2796 - val_accuracy: 0.9221\n",
      "Epoch 151/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2833 - accuracy: 0.9211 - val_loss: 0.2795 - val_accuracy: 0.9221\n",
      "Epoch 152/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2832 - accuracy: 0.9208 - val_loss: 0.2794 - val_accuracy: 0.9224\n",
      "Epoch 153/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2830 - accuracy: 0.9212 - val_loss: 0.2793 - val_accuracy: 0.9224\n",
      "Epoch 154/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2828 - accuracy: 0.9213 - val_loss: 0.2791 - val_accuracy: 0.9227\n",
      "Epoch 155/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2827 - accuracy: 0.9210 - val_loss: 0.2790 - val_accuracy: 0.9227\n",
      "Epoch 156/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2825 - accuracy: 0.9210 - val_loss: 0.2789 - val_accuracy: 0.9224\n",
      "Epoch 157/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2823 - accuracy: 0.9213 - val_loss: 0.2788 - val_accuracy: 0.9229\n",
      "Epoch 158/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2821 - accuracy: 0.9214 - val_loss: 0.2786 - val_accuracy: 0.9229\n",
      "Epoch 159/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2820 - accuracy: 0.9214 - val_loss: 0.2785 - val_accuracy: 0.9228\n",
      "Epoch 160/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2818 - accuracy: 0.9212 - val_loss: 0.2785 - val_accuracy: 0.9228\n",
      "Epoch 161/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2817 - accuracy: 0.9213 - val_loss: 0.2784 - val_accuracy: 0.9230\n",
      "Epoch 162/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2815 - accuracy: 0.9215 - val_loss: 0.2783 - val_accuracy: 0.9230\n",
      "Epoch 163/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2813 - accuracy: 0.9216 - val_loss: 0.2782 - val_accuracy: 0.9231\n",
      "Epoch 164/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2812 - accuracy: 0.9216 - val_loss: 0.2781 - val_accuracy: 0.9222\n",
      "Epoch 165/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2810 - accuracy: 0.9215 - val_loss: 0.2780 - val_accuracy: 0.9228\n",
      "Epoch 166/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2809 - accuracy: 0.9214 - val_loss: 0.2779 - val_accuracy: 0.9227\n",
      "Epoch 167/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2807 - accuracy: 0.9216 - val_loss: 0.2778 - val_accuracy: 0.9226\n",
      "Epoch 168/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2805 - accuracy: 0.9216 - val_loss: 0.2778 - val_accuracy: 0.9225\n",
      "Epoch 169/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2804 - accuracy: 0.9216 - val_loss: 0.2776 - val_accuracy: 0.9227\n",
      "Epoch 170/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2802 - accuracy: 0.9220 - val_loss: 0.2775 - val_accuracy: 0.9226\n",
      "Epoch 171/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2801 - accuracy: 0.9218 - val_loss: 0.2775 - val_accuracy: 0.9226\n",
      "Epoch 172/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2799 - accuracy: 0.9218 - val_loss: 0.2774 - val_accuracy: 0.9231\n",
      "Epoch 173/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2798 - accuracy: 0.9217 - val_loss: 0.2772 - val_accuracy: 0.9228\n",
      "Epoch 174/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2796 - accuracy: 0.9219 - val_loss: 0.2771 - val_accuracy: 0.9230\n",
      "Epoch 175/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2795 - accuracy: 0.9219 - val_loss: 0.2771 - val_accuracy: 0.9230\n",
      "Epoch 176/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2794 - accuracy: 0.9221 - val_loss: 0.2770 - val_accuracy: 0.9228\n",
      "Epoch 177/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2792 - accuracy: 0.9220 - val_loss: 0.2769 - val_accuracy: 0.9227\n",
      "Epoch 178/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2790 - accuracy: 0.9218 - val_loss: 0.2768 - val_accuracy: 0.9226\n",
      "Epoch 179/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2789 - accuracy: 0.9222 - val_loss: 0.2768 - val_accuracy: 0.9227\n",
      "Epoch 180/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2788 - accuracy: 0.9223 - val_loss: 0.2766 - val_accuracy: 0.9228\n",
      "Epoch 181/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2786 - accuracy: 0.9222 - val_loss: 0.2765 - val_accuracy: 0.9227\n",
      "Epoch 182/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2785 - accuracy: 0.9222 - val_loss: 0.2765 - val_accuracy: 0.9229\n",
      "Epoch 183/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2783 - accuracy: 0.9222 - val_loss: 0.2764 - val_accuracy: 0.9231\n",
      "Epoch 184/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2782 - accuracy: 0.9223 - val_loss: 0.2763 - val_accuracy: 0.9227\n",
      "Epoch 185/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2781 - accuracy: 0.9224 - val_loss: 0.2762 - val_accuracy: 0.9234\n",
      "Epoch 186/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2780 - accuracy: 0.9225 - val_loss: 0.2762 - val_accuracy: 0.9228\n",
      "Epoch 187/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2778 - accuracy: 0.9223 - val_loss: 0.2761 - val_accuracy: 0.9227\n",
      "Epoch 188/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2777 - accuracy: 0.9223 - val_loss: 0.2760 - val_accuracy: 0.9227\n",
      "Epoch 189/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2776 - accuracy: 0.9224 - val_loss: 0.2759 - val_accuracy: 0.9232\n",
      "Epoch 190/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2774 - accuracy: 0.9225 - val_loss: 0.2758 - val_accuracy: 0.9231\n",
      "Epoch 191/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2773 - accuracy: 0.9226 - val_loss: 0.2758 - val_accuracy: 0.9233\n",
      "Epoch 192/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2772 - accuracy: 0.9227 - val_loss: 0.2757 - val_accuracy: 0.9229\n",
      "Epoch 193/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2770 - accuracy: 0.9225 - val_loss: 0.2757 - val_accuracy: 0.9233\n",
      "Epoch 194/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2769 - accuracy: 0.9229 - val_loss: 0.2756 - val_accuracy: 0.9231\n",
      "Epoch 195/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2768 - accuracy: 0.9228 - val_loss: 0.2755 - val_accuracy: 0.9233\n",
      "Epoch 196/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2767 - accuracy: 0.9228 - val_loss: 0.2755 - val_accuracy: 0.9230\n",
      "Epoch 197/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2765 - accuracy: 0.9230 - val_loss: 0.2754 - val_accuracy: 0.9231\n",
      "Epoch 198/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2764 - accuracy: 0.9229 - val_loss: 0.2753 - val_accuracy: 0.9228\n",
      "Epoch 199/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2763 - accuracy: 0.9228 - val_loss: 0.2753 - val_accuracy: 0.9231\n",
      "Epoch 200/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2762 - accuracy: 0.9231 - val_loss: 0.2751 - val_accuracy: 0.9227\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.2780 - accuracy: 0.9227\n",
      "\n",
      "Test score: 0.2779505252838135\n",
      "Test accuracy: 0.9226999878883362\n"
     ]
    }
   ],
   "source": [
    "# Import dos Pacotes\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Garantindo que o resultado pode ser reproduzido\n",
    "np.random.seed(100)\n",
    "\n",
    "# Parâmetros da rede e do treinamento\n",
    "\n",
    "NB_EPOCH = 200 #Número de épocas\n",
    "\n",
    "BATCH_SIZE = 128 #Tamanho do Batch\n",
    "\n",
    "VERBOSE = 1 #Verbose\n",
    "\n",
    "NB_CLASSES = 10   #Número de outputs = número de dígitos\n",
    "\n",
    "OPTIMIZER = tensorflow.keras.optimizers.SGD() #Otimizador SGD\n",
    "\n",
    "N_HIDDEN = 128  #Número de neurônios ocultos\n",
    "\n",
    "VALIDATION_SPLIT = 0.2 #fração de validação \n",
    "\n",
    "# Gerando datasets de treino e e teste\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# X_train possui 60000 linhas de valores 28x28 --> reshape para 60000 x 784\n",
    "# Gera versão final dos datasetes de treino e de teste\n",
    "RESHAPED = 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalizando os dados\n",
    "# Tipicamente, os valores associados a cada pixel são normalizados na faixa [0, 1] \n",
    "# (o que significa que a intensidade de cada pixel é dividida por 255, o valor de intensidade máxima). \n",
    "# A saída são 10 classes, uma para cada dígito.\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'exemplos de treino')\n",
    "print(X_test.shape[0], 'exemplos de teste')\n",
    "\n",
    "# Converte os vetores da class para matrizes binárias das classes\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# Cria as camadas\n",
    "# A camada final usa a função de ativação softmax, uma generalização da função sigmóide. \n",
    "# Softmax transforma um vetor k-dimensional de valores reais arbitrários em um vetor k-dimensional de valores reais no \n",
    "# intervalo (0, 1).\n",
    "model = Sequential()\n",
    "model.add(Dense(NB_CLASSES, input_shape = (RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Sumário\n",
    "model.summary()\n",
    "\n",
    "# Compila o modelo\n",
    "# Precisamos selecionar o otimizador que é o algoritmo específico usado para atualizar pesos enquanto \n",
    "# treinamos nosso modelo.\n",
    "# Precisamos selecionar também a função objetivo que é usada pelo otimizador para navegar no espaço de pesos \n",
    "# (frequentemente, as funções objetivo são chamadas de função de perda (loss) e o processo de otimização é definido \n",
    "# como um processo de minimização de perdas).\n",
    "# Outras funções aqui: https://keras.io/losses/\n",
    "# A função objetivo \"categorical_crossentropy\" é a função objetivo adequada para predições de rótulos multiclass. \n",
    "# É também a escolha padrão em associação com a ativação softmax.\n",
    "# A métrica é usada para medir a performance do modelo. Outras métricas: https://keras.io/metrics/\n",
    "# As métricas são semelhantes às funções objetivo, com a única diferença de que elas não são usadas para \n",
    "# treinar um modelo, mas apenas para avaliar um modelo. \n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = OPTIMIZER, metrics = ['accuracy'])\n",
    "\n",
    "# Treina o modelo\n",
    "# Epochs: Este é o número de vezes que o modelo é exposto ao conjunto de treinamento. Em cada iteração, \n",
    "# o otimizador tenta ajustar os pesos para que a função objetivo seja minimizada. \n",
    "# Batch_size: Esse é o número de instâncias de treinamento observadas antes que o otimizador execute uma \n",
    "# atualização de peso.\n",
    "# Reservamos parte do conjunto de treinamento para validação. A idéia chave é que reservamos uma parte dos \n",
    "# dados de treinamento para medir o desempenho na validação durante o treinamento. \n",
    "\n",
    "modelo_v1 = model.fit(X_train, Y_train,\n",
    "                      batch_size = BATCH_SIZE, \n",
    "                      epochs = NB_EPOCH,\n",
    "                      verbose = VERBOSE, \n",
    "                      validation_split = VALIDATION_SPLIT)\n",
    "\n",
    "# Avalia o modelo com os dados de teste\n",
    "# Uma vez treinado o modelo, podemos avaliá-lo no conjunto de testes que contém novos exemplos não vistos. \n",
    "# Desta forma, podemos obter o valor mínimo alcançado pela função objetivo e o melhor valor alcançado pela métrica \n",
    "# de avaliação. Note-se que o conjunto de treinamento e o conjunto de teste são rigorosamente separados. \n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose = VERBOSE)\n",
    "\n",
    "# Imprime a perda e a acurácia\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron - Versão 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adiciona 2 camadas ocultas, usando função de ativação ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 exemplos de treino\n",
      "10000 exemplos de teste\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.4125 - accuracy: 0.6490 - val_loss: 0.7090 - val_accuracy: 0.8378\n",
      "Epoch 2/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5727 - accuracy: 0.8566 - val_loss: 0.4415 - val_accuracy: 0.8857\n",
      "Epoch 3/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4257 - accuracy: 0.8866 - val_loss: 0.3653 - val_accuracy: 0.8999\n",
      "Epoch 4/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3702 - accuracy: 0.8974 - val_loss: 0.3289 - val_accuracy: 0.9078\n",
      "Epoch 5/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3385 - accuracy: 0.9046 - val_loss: 0.3080 - val_accuracy: 0.9128\n",
      "Epoch 6/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3173 - accuracy: 0.9107 - val_loss: 0.2904 - val_accuracy: 0.9177\n",
      "Epoch 7/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3003 - accuracy: 0.9148 - val_loss: 0.2788 - val_accuracy: 0.9222\n",
      "Epoch 8/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2866 - accuracy: 0.9190 - val_loss: 0.2666 - val_accuracy: 0.9241\n",
      "Epoch 9/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2745 - accuracy: 0.9222 - val_loss: 0.2565 - val_accuracy: 0.9285\n",
      "Epoch 10/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2639 - accuracy: 0.9249 - val_loss: 0.2491 - val_accuracy: 0.9305\n",
      "Epoch 11/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2547 - accuracy: 0.9281 - val_loss: 0.2411 - val_accuracy: 0.9312\n",
      "Epoch 12/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2459 - accuracy: 0.9305 - val_loss: 0.2337 - val_accuracy: 0.9337\n",
      "Epoch 13/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2379 - accuracy: 0.9320 - val_loss: 0.2272 - val_accuracy: 0.9366\n",
      "Epoch 14/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2304 - accuracy: 0.9344 - val_loss: 0.2219 - val_accuracy: 0.9378\n",
      "Epoch 15/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2235 - accuracy: 0.9371 - val_loss: 0.2160 - val_accuracy: 0.9390\n",
      "Epoch 16/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2166 - accuracy: 0.9382 - val_loss: 0.2121 - val_accuracy: 0.9402\n",
      "Epoch 17/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2102 - accuracy: 0.9400 - val_loss: 0.2055 - val_accuracy: 0.9426\n",
      "Epoch 18/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2044 - accuracy: 0.9416 - val_loss: 0.2002 - val_accuracy: 0.9456\n",
      "Epoch 19/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1987 - accuracy: 0.9429 - val_loss: 0.1955 - val_accuracy: 0.9462\n",
      "Epoch 20/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1933 - accuracy: 0.9448 - val_loss: 0.1926 - val_accuracy: 0.9473\n",
      "Epoch 21/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1882 - accuracy: 0.9457 - val_loss: 0.1871 - val_accuracy: 0.9493\n",
      "Epoch 22/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1833 - accuracy: 0.9470 - val_loss: 0.1850 - val_accuracy: 0.9478\n",
      "Epoch 23/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1788 - accuracy: 0.9481 - val_loss: 0.1799 - val_accuracy: 0.9506\n",
      "Epoch 24/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1742 - accuracy: 0.9495 - val_loss: 0.1774 - val_accuracy: 0.9515\n",
      "Epoch 25/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1700 - accuracy: 0.9514 - val_loss: 0.1736 - val_accuracy: 0.9517\n",
      "Epoch 26/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1660 - accuracy: 0.9523 - val_loss: 0.1696 - val_accuracy: 0.9532\n",
      "Epoch 27/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1617 - accuracy: 0.9534 - val_loss: 0.1661 - val_accuracy: 0.9535\n",
      "Epoch 28/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1583 - accuracy: 0.9543 - val_loss: 0.1645 - val_accuracy: 0.9538\n",
      "Epoch 29/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1548 - accuracy: 0.9556 - val_loss: 0.1610 - val_accuracy: 0.9549\n",
      "Epoch 30/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1512 - accuracy: 0.9564 - val_loss: 0.1592 - val_accuracy: 0.9542\n",
      "Epoch 31/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1479 - accuracy: 0.9570 - val_loss: 0.1569 - val_accuracy: 0.9570\n",
      "Epoch 32/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1447 - accuracy: 0.9585 - val_loss: 0.1546 - val_accuracy: 0.9572\n",
      "Epoch 33/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1415 - accuracy: 0.9591 - val_loss: 0.1546 - val_accuracy: 0.9572\n",
      "Epoch 34/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1387 - accuracy: 0.9599 - val_loss: 0.1491 - val_accuracy: 0.9582\n",
      "Epoch 35/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1356 - accuracy: 0.9609 - val_loss: 0.1467 - val_accuracy: 0.9599\n",
      "Epoch 36/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1330 - accuracy: 0.9616 - val_loss: 0.1451 - val_accuracy: 0.9601\n",
      "Epoch 37/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1302 - accuracy: 0.9626 - val_loss: 0.1434 - val_accuracy: 0.9589\n",
      "Epoch 38/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1278 - accuracy: 0.9633 - val_loss: 0.1420 - val_accuracy: 0.9599\n",
      "Epoch 39/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1252 - accuracy: 0.9639 - val_loss: 0.1394 - val_accuracy: 0.9609\n",
      "Epoch 40/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1227 - accuracy: 0.9647 - val_loss: 0.1382 - val_accuracy: 0.9617\n",
      "Epoch 41/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1205 - accuracy: 0.9656 - val_loss: 0.1361 - val_accuracy: 0.9632\n",
      "Epoch 42/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1184 - accuracy: 0.9661 - val_loss: 0.1348 - val_accuracy: 0.9624\n",
      "Epoch 43/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1159 - accuracy: 0.9672 - val_loss: 0.1328 - val_accuracy: 0.9639\n",
      "Epoch 44/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1138 - accuracy: 0.9677 - val_loss: 0.1315 - val_accuracy: 0.9632\n",
      "Epoch 45/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1119 - accuracy: 0.9679 - val_loss: 0.1307 - val_accuracy: 0.9640\n",
      "Epoch 46/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1097 - accuracy: 0.9686 - val_loss: 0.1298 - val_accuracy: 0.9641\n",
      "Epoch 47/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1077 - accuracy: 0.9695 - val_loss: 0.1274 - val_accuracy: 0.9652\n",
      "Epoch 48/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1060 - accuracy: 0.9703 - val_loss: 0.1264 - val_accuracy: 0.9650\n",
      "Epoch 49/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1042 - accuracy: 0.9707 - val_loss: 0.1252 - val_accuracy: 0.9657\n",
      "Epoch 50/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1022 - accuracy: 0.9717 - val_loss: 0.1237 - val_accuracy: 0.9669\n",
      "Epoch 51/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1005 - accuracy: 0.9715 - val_loss: 0.1225 - val_accuracy: 0.9666\n",
      "Epoch 52/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0990 - accuracy: 0.9724 - val_loss: 0.1221 - val_accuracy: 0.9661\n",
      "Epoch 53/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0972 - accuracy: 0.9729 - val_loss: 0.1204 - val_accuracy: 0.9668\n",
      "Epoch 54/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0956 - accuracy: 0.9732 - val_loss: 0.1198 - val_accuracy: 0.9669\n",
      "Epoch 55/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0940 - accuracy: 0.9739 - val_loss: 0.1186 - val_accuracy: 0.9674\n",
      "Epoch 56/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0924 - accuracy: 0.9747 - val_loss: 0.1175 - val_accuracy: 0.9669\n",
      "Epoch 57/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0911 - accuracy: 0.9747 - val_loss: 0.1170 - val_accuracy: 0.9669\n",
      "Epoch 58/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0895 - accuracy: 0.9755 - val_loss: 0.1172 - val_accuracy: 0.9668\n",
      "Epoch 59/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0882 - accuracy: 0.9758 - val_loss: 0.1147 - val_accuracy: 0.9672\n",
      "Epoch 60/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0868 - accuracy: 0.9761 - val_loss: 0.1137 - val_accuracy: 0.9680\n",
      "Epoch 61/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0854 - accuracy: 0.9764 - val_loss: 0.1132 - val_accuracy: 0.9685\n",
      "Epoch 62/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0842 - accuracy: 0.9769 - val_loss: 0.1117 - val_accuracy: 0.9676\n",
      "Epoch 63/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0828 - accuracy: 0.9771 - val_loss: 0.1120 - val_accuracy: 0.9678\n",
      "Epoch 64/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0814 - accuracy: 0.9780 - val_loss: 0.1106 - val_accuracy: 0.9684\n",
      "Epoch 65/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0804 - accuracy: 0.9780 - val_loss: 0.1105 - val_accuracy: 0.9689\n",
      "Epoch 66/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0790 - accuracy: 0.9786 - val_loss: 0.1091 - val_accuracy: 0.9681\n",
      "Epoch 67/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0778 - accuracy: 0.9787 - val_loss: 0.1082 - val_accuracy: 0.9682\n",
      "Epoch 68/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0766 - accuracy: 0.9791 - val_loss: 0.1079 - val_accuracy: 0.9682\n",
      "Epoch 69/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0755 - accuracy: 0.9791 - val_loss: 0.1078 - val_accuracy: 0.9690\n",
      "Epoch 70/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0746 - accuracy: 0.9796 - val_loss: 0.1069 - val_accuracy: 0.9693\n",
      "Epoch 71/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0733 - accuracy: 0.9803 - val_loss: 0.1053 - val_accuracy: 0.9693\n",
      "Epoch 72/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0722 - accuracy: 0.9806 - val_loss: 0.1056 - val_accuracy: 0.9688\n",
      "Epoch 73/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0712 - accuracy: 0.9806 - val_loss: 0.1043 - val_accuracy: 0.9697\n",
      "Epoch 74/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0703 - accuracy: 0.9806 - val_loss: 0.1040 - val_accuracy: 0.9694\n",
      "Epoch 75/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0693 - accuracy: 0.9810 - val_loss: 0.1033 - val_accuracy: 0.9701\n",
      "Epoch 76/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0683 - accuracy: 0.9814 - val_loss: 0.1029 - val_accuracy: 0.9703\n",
      "Epoch 77/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0673 - accuracy: 0.9821 - val_loss: 0.1021 - val_accuracy: 0.9703\n",
      "Epoch 78/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0664 - accuracy: 0.9821 - val_loss: 0.1017 - val_accuracy: 0.9699\n",
      "Epoch 79/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0654 - accuracy: 0.9826 - val_loss: 0.1017 - val_accuracy: 0.9703\n",
      "Epoch 80/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0646 - accuracy: 0.9826 - val_loss: 0.1006 - val_accuracy: 0.9701\n",
      "Epoch 81/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0636 - accuracy: 0.9830 - val_loss: 0.1010 - val_accuracy: 0.9704\n",
      "Epoch 82/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0628 - accuracy: 0.9832 - val_loss: 0.1001 - val_accuracy: 0.9703\n",
      "Epoch 83/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0618 - accuracy: 0.9839 - val_loss: 0.0995 - val_accuracy: 0.9707\n",
      "Epoch 84/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0610 - accuracy: 0.9838 - val_loss: 0.0986 - val_accuracy: 0.9710\n",
      "Epoch 85/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0603 - accuracy: 0.9840 - val_loss: 0.0986 - val_accuracy: 0.9711\n",
      "Epoch 86/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0595 - accuracy: 0.9842 - val_loss: 0.0981 - val_accuracy: 0.9715\n",
      "Epoch 87/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0585 - accuracy: 0.9841 - val_loss: 0.0975 - val_accuracy: 0.9711\n",
      "Epoch 88/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0579 - accuracy: 0.9846 - val_loss: 0.0969 - val_accuracy: 0.9708\n",
      "Epoch 89/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0571 - accuracy: 0.9850 - val_loss: 0.0961 - val_accuracy: 0.9708\n",
      "Epoch 90/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0563 - accuracy: 0.9852 - val_loss: 0.0964 - val_accuracy: 0.9712\n",
      "Epoch 91/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0557 - accuracy: 0.9851 - val_loss: 0.0969 - val_accuracy: 0.9715\n",
      "Epoch 92/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0550 - accuracy: 0.9856 - val_loss: 0.0954 - val_accuracy: 0.9720\n",
      "Epoch 93/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0542 - accuracy: 0.9862 - val_loss: 0.0961 - val_accuracy: 0.9722\n",
      "Epoch 94/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0535 - accuracy: 0.9860 - val_loss: 0.0952 - val_accuracy: 0.9713\n",
      "Epoch 95/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0528 - accuracy: 0.9860 - val_loss: 0.0948 - val_accuracy: 0.9712\n",
      "Epoch 96/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0521 - accuracy: 0.9864 - val_loss: 0.0942 - val_accuracy: 0.9713\n",
      "Epoch 97/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0513 - accuracy: 0.9867 - val_loss: 0.0953 - val_accuracy: 0.9716\n",
      "Epoch 98/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0508 - accuracy: 0.9867 - val_loss: 0.0944 - val_accuracy: 0.9708\n",
      "Epoch 99/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0502 - accuracy: 0.9871 - val_loss: 0.0936 - val_accuracy: 0.9715\n",
      "Epoch 100/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0495 - accuracy: 0.9872 - val_loss: 0.0939 - val_accuracy: 0.9718\n",
      "Epoch 101/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0489 - accuracy: 0.9872 - val_loss: 0.0940 - val_accuracy: 0.9721\n",
      "Epoch 102/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0482 - accuracy: 0.9878 - val_loss: 0.0940 - val_accuracy: 0.9718\n",
      "Epoch 103/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0476 - accuracy: 0.9880 - val_loss: 0.0924 - val_accuracy: 0.9717\n",
      "Epoch 104/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0471 - accuracy: 0.9882 - val_loss: 0.0933 - val_accuracy: 0.9719\n",
      "Epoch 105/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0463 - accuracy: 0.9882 - val_loss: 0.0923 - val_accuracy: 0.9726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0459 - accuracy: 0.9882 - val_loss: 0.0924 - val_accuracy: 0.9722\n",
      "Epoch 107/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0452 - accuracy: 0.9886 - val_loss: 0.0925 - val_accuracy: 0.9718\n",
      "Epoch 108/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0448 - accuracy: 0.9887 - val_loss: 0.0919 - val_accuracy: 0.9718\n",
      "Epoch 109/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0442 - accuracy: 0.9890 - val_loss: 0.0918 - val_accuracy: 0.9713\n",
      "Epoch 110/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0437 - accuracy: 0.9891 - val_loss: 0.0919 - val_accuracy: 0.9720\n",
      "Epoch 111/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0432 - accuracy: 0.9896 - val_loss: 0.0908 - val_accuracy: 0.9720\n",
      "Epoch 112/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0426 - accuracy: 0.9895 - val_loss: 0.0908 - val_accuracy: 0.9728\n",
      "Epoch 113/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0421 - accuracy: 0.9898 - val_loss: 0.0915 - val_accuracy: 0.9721\n",
      "Epoch 114/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0416 - accuracy: 0.9896 - val_loss: 0.0912 - val_accuracy: 0.9727\n",
      "Epoch 115/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0410 - accuracy: 0.9900 - val_loss: 0.0900 - val_accuracy: 0.9723\n",
      "Epoch 116/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0405 - accuracy: 0.9902 - val_loss: 0.0906 - val_accuracy: 0.9718\n",
      "Epoch 117/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0401 - accuracy: 0.9902 - val_loss: 0.0902 - val_accuracy: 0.9723\n",
      "Epoch 118/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0396 - accuracy: 0.9905 - val_loss: 0.0901 - val_accuracy: 0.9718\n",
      "Epoch 119/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0392 - accuracy: 0.9906 - val_loss: 0.0900 - val_accuracy: 0.9728\n",
      "Epoch 120/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0386 - accuracy: 0.9909 - val_loss: 0.0895 - val_accuracy: 0.9727\n",
      "Epoch 121/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0383 - accuracy: 0.9910 - val_loss: 0.0894 - val_accuracy: 0.9729\n",
      "Epoch 122/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0378 - accuracy: 0.9909 - val_loss: 0.0898 - val_accuracy: 0.9727\n",
      "Epoch 123/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0373 - accuracy: 0.9913 - val_loss: 0.0890 - val_accuracy: 0.9730\n",
      "Epoch 124/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0369 - accuracy: 0.9915 - val_loss: 0.0906 - val_accuracy: 0.9723\n",
      "Epoch 125/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0364 - accuracy: 0.9917 - val_loss: 0.0899 - val_accuracy: 0.9731\n",
      "Epoch 126/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0360 - accuracy: 0.9917 - val_loss: 0.0889 - val_accuracy: 0.9729\n",
      "Epoch 127/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0356 - accuracy: 0.9916 - val_loss: 0.0893 - val_accuracy: 0.9730\n",
      "Epoch 128/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0351 - accuracy: 0.9919 - val_loss: 0.0889 - val_accuracy: 0.9733\n",
      "Epoch 129/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0347 - accuracy: 0.9921 - val_loss: 0.0886 - val_accuracy: 0.9728\n",
      "Epoch 130/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0344 - accuracy: 0.9921 - val_loss: 0.0886 - val_accuracy: 0.9733\n",
      "Epoch 131/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0339 - accuracy: 0.9924 - val_loss: 0.0881 - val_accuracy: 0.9730\n",
      "Epoch 132/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0336 - accuracy: 0.9923 - val_loss: 0.0894 - val_accuracy: 0.9725\n",
      "Epoch 133/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0332 - accuracy: 0.9925 - val_loss: 0.0878 - val_accuracy: 0.9732\n",
      "Epoch 134/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0328 - accuracy: 0.9926 - val_loss: 0.0885 - val_accuracy: 0.9727\n",
      "Epoch 135/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0324 - accuracy: 0.9928 - val_loss: 0.0880 - val_accuracy: 0.9734\n",
      "Epoch 136/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0321 - accuracy: 0.9928 - val_loss: 0.0886 - val_accuracy: 0.9725\n",
      "Epoch 137/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0317 - accuracy: 0.9929 - val_loss: 0.0879 - val_accuracy: 0.9737\n",
      "Epoch 138/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0313 - accuracy: 0.9932 - val_loss: 0.0879 - val_accuracy: 0.9729\n",
      "Epoch 139/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0310 - accuracy: 0.9931 - val_loss: 0.0880 - val_accuracy: 0.9732\n",
      "Epoch 140/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0306 - accuracy: 0.9934 - val_loss: 0.0878 - val_accuracy: 0.9731\n",
      "Epoch 141/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0303 - accuracy: 0.9933 - val_loss: 0.0878 - val_accuracy: 0.9728\n",
      "Epoch 142/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0299 - accuracy: 0.9935 - val_loss: 0.0883 - val_accuracy: 0.9736\n",
      "Epoch 143/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0295 - accuracy: 0.9937 - val_loss: 0.0874 - val_accuracy: 0.9735\n",
      "Epoch 144/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0292 - accuracy: 0.9938 - val_loss: 0.0876 - val_accuracy: 0.9735\n",
      "Epoch 145/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0289 - accuracy: 0.9937 - val_loss: 0.0876 - val_accuracy: 0.9733\n",
      "Epoch 146/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0285 - accuracy: 0.9939 - val_loss: 0.0874 - val_accuracy: 0.9734\n",
      "Epoch 147/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0282 - accuracy: 0.9940 - val_loss: 0.0874 - val_accuracy: 0.9736\n",
      "Epoch 148/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0279 - accuracy: 0.9942 - val_loss: 0.0879 - val_accuracy: 0.9732\n",
      "Epoch 149/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0276 - accuracy: 0.9942 - val_loss: 0.0875 - val_accuracy: 0.9738\n",
      "Epoch 150/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0273 - accuracy: 0.9944 - val_loss: 0.0874 - val_accuracy: 0.9735\n",
      "Epoch 151/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0269 - accuracy: 0.9945 - val_loss: 0.0876 - val_accuracy: 0.9734\n",
      "Epoch 152/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0267 - accuracy: 0.9947 - val_loss: 0.0873 - val_accuracy: 0.9738\n",
      "Epoch 153/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0263 - accuracy: 0.9948 - val_loss: 0.0876 - val_accuracy: 0.9737\n",
      "Epoch 154/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0261 - accuracy: 0.9947 - val_loss: 0.0873 - val_accuracy: 0.9734\n",
      "Epoch 155/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0258 - accuracy: 0.9948 - val_loss: 0.0874 - val_accuracy: 0.9734\n",
      "Epoch 156/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0255 - accuracy: 0.9952 - val_loss: 0.0875 - val_accuracy: 0.9731\n",
      "Epoch 157/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0252 - accuracy: 0.9950 - val_loss: 0.0873 - val_accuracy: 0.9738\n",
      "Epoch 158/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0250 - accuracy: 0.9954 - val_loss: 0.0878 - val_accuracy: 0.9736\n",
      "Epoch 159/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0246 - accuracy: 0.9950 - val_loss: 0.0873 - val_accuracy: 0.9740\n",
      "Epoch 160/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0244 - accuracy: 0.9953 - val_loss: 0.0876 - val_accuracy: 0.9737\n",
      "Epoch 161/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0241 - accuracy: 0.9954 - val_loss: 0.0882 - val_accuracy: 0.9734\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0239 - accuracy: 0.9957 - val_loss: 0.0882 - val_accuracy: 0.9731\n",
      "Epoch 163/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0236 - accuracy: 0.9954 - val_loss: 0.0872 - val_accuracy: 0.9735\n",
      "Epoch 164/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0233 - accuracy: 0.9956 - val_loss: 0.0873 - val_accuracy: 0.9739\n",
      "Epoch 165/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0231 - accuracy: 0.9958 - val_loss: 0.0871 - val_accuracy: 0.9735\n",
      "Epoch 166/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0229 - accuracy: 0.9958 - val_loss: 0.0873 - val_accuracy: 0.9735\n",
      "Epoch 167/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0227 - accuracy: 0.9960 - val_loss: 0.0875 - val_accuracy: 0.9736\n",
      "Epoch 168/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0224 - accuracy: 0.9960 - val_loss: 0.0870 - val_accuracy: 0.9738\n",
      "Epoch 169/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0221 - accuracy: 0.9958 - val_loss: 0.0872 - val_accuracy: 0.9736\n",
      "Epoch 170/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0219 - accuracy: 0.9960 - val_loss: 0.0871 - val_accuracy: 0.9734\n",
      "Epoch 171/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0216 - accuracy: 0.9962 - val_loss: 0.0870 - val_accuracy: 0.9733\n",
      "Epoch 172/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0214 - accuracy: 0.9963 - val_loss: 0.0874 - val_accuracy: 0.9736\n",
      "Epoch 173/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0212 - accuracy: 0.9963 - val_loss: 0.0873 - val_accuracy: 0.9737\n",
      "Epoch 174/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0210 - accuracy: 0.9964 - val_loss: 0.0872 - val_accuracy: 0.9740\n",
      "Epoch 175/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0208 - accuracy: 0.9964 - val_loss: 0.0880 - val_accuracy: 0.9732\n",
      "Epoch 176/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0205 - accuracy: 0.9964 - val_loss: 0.0874 - val_accuracy: 0.9736\n",
      "Epoch 177/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0202 - accuracy: 0.9965 - val_loss: 0.0876 - val_accuracy: 0.9735\n",
      "Epoch 178/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0201 - accuracy: 0.9966 - val_loss: 0.0872 - val_accuracy: 0.9736\n",
      "Epoch 179/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0198 - accuracy: 0.9967 - val_loss: 0.0875 - val_accuracy: 0.9734\n",
      "Epoch 180/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0197 - accuracy: 0.9967 - val_loss: 0.0875 - val_accuracy: 0.9737\n",
      "Epoch 181/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0195 - accuracy: 0.9968 - val_loss: 0.0872 - val_accuracy: 0.9735\n",
      "Epoch 182/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0193 - accuracy: 0.9969 - val_loss: 0.0872 - val_accuracy: 0.9738\n",
      "Epoch 183/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0190 - accuracy: 0.9966 - val_loss: 0.0877 - val_accuracy: 0.9741\n",
      "Epoch 184/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0188 - accuracy: 0.9969 - val_loss: 0.0874 - val_accuracy: 0.9738\n",
      "Epoch 185/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0186 - accuracy: 0.9968 - val_loss: 0.0876 - val_accuracy: 0.9735\n",
      "Epoch 186/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9971 - val_loss: 0.0877 - val_accuracy: 0.9735\n",
      "Epoch 187/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0183 - accuracy: 0.9971 - val_loss: 0.0872 - val_accuracy: 0.9743\n",
      "Epoch 188/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9972 - val_loss: 0.0873 - val_accuracy: 0.9736\n",
      "Epoch 189/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0178 - accuracy: 0.9972 - val_loss: 0.0875 - val_accuracy: 0.9737\n",
      "Epoch 190/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0176 - accuracy: 0.9973 - val_loss: 0.0876 - val_accuracy: 0.9735\n",
      "Epoch 191/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9974 - val_loss: 0.0878 - val_accuracy: 0.9740\n",
      "Epoch 192/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0172 - accuracy: 0.9973 - val_loss: 0.0876 - val_accuracy: 0.9741\n",
      "Epoch 193/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0171 - accuracy: 0.9974 - val_loss: 0.0877 - val_accuracy: 0.9738\n",
      "Epoch 194/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0169 - accuracy: 0.9975 - val_loss: 0.0878 - val_accuracy: 0.9735\n",
      "Epoch 195/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0168 - accuracy: 0.9975 - val_loss: 0.0877 - val_accuracy: 0.9737\n",
      "Epoch 196/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0166 - accuracy: 0.9975 - val_loss: 0.0880 - val_accuracy: 0.9737\n",
      "Epoch 197/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0164 - accuracy: 0.9975 - val_loss: 0.0885 - val_accuracy: 0.9734\n",
      "Epoch 198/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0162 - accuracy: 0.9977 - val_loss: 0.0877 - val_accuracy: 0.9732\n",
      "Epoch 199/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0161 - accuracy: 0.9977 - val_loss: 0.0879 - val_accuracy: 0.9734\n",
      "Epoch 200/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0159 - accuracy: 0.9977 - val_loss: 0.0877 - val_accuracy: 0.9739\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0767 - accuracy: 0.9781\n",
      "\n",
      "Test score: 0.07673961669206619\n",
      "Test accuracy: 0.9781000018119812\n"
     ]
    }
   ],
   "source": [
    "# Import dos Pacotes\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Garantindo que o resultado pode ser reproduzido\n",
    "np.random.seed(100)  \n",
    "\n",
    "# Parâmetros da rede e do treinamento\n",
    "\n",
    "NB_EPOCH = 200 #Número de épocas\n",
    "\n",
    "BATCH_SIZE = 128 #Tamanho do Batch\n",
    "\n",
    "VERBOSE = 1 #Verbose\n",
    "\n",
    "NB_CLASSES = 10   #Número de outputs = número de dígitos\n",
    "\n",
    "OPTIMIZER = tensorflow.keras.optimizers.SGD() #Otimizador SGD\n",
    "\n",
    "N_HIDDEN = 128  #Número de neurônios ocultos\n",
    "\n",
    "VALIDATION_SPLIT = 0.2 #fração de validação\n",
    "\n",
    "# Gerando datasets de treino e e teste\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# X_train possui 60000 linhas de valores 28x28 --> reshape para 60000 x 784\n",
    "# Gera versão final dos datasetes de treino e de teste\n",
    "RESHAPED = 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalizando os dados\n",
    "# Tipicamente, os valores associados a cada pixel são normalizados na faixa [0, 1] \n",
    "# (o que significa que a intensidade de cada pixel é dividida por 255, o valor de intensidade máxima). \n",
    "# A saída é 10 classes, uma para cada dígito.\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'exemplos de treino')\n",
    "print(X_test.shape[0], 'exemplos de teste')\n",
    "\n",
    "# Converte os vetores da class para matrizes binárias das classes\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# Cria as camadas\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape = (RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Sumário\n",
    "model.summary()\n",
    "\n",
    "# Compila o modelo\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = OPTIMIZER, metrics = ['accuracy'])\n",
    "\n",
    "# Treina o modelo\n",
    "modelo_v2 = model.fit(X_train, Y_train,\n",
    "                      batch_size = BATCH_SIZE, \n",
    "                      epochs = NB_EPOCH,\n",
    "                      verbose = VERBOSE, \n",
    "                      validation_split = VALIDATION_SPLIT)\n",
    "\n",
    "# Avalia o modelo com os dados de teste\n",
    "score = model.evaluate(X_test, Y_test, verbose = VERBOSE)\n",
    "\n",
    "# Imprime a perda e a acurácia\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron - Versão 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionando Dropout nas camadas ocultas e aumentando o número de epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 exemplos de treino\n",
      "10000 exemplos de teste\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 1.6924 - accuracy: 0.4686 - val_loss: 0.8729 - val_accuracy: 0.8189\n",
      "Epoch 2/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.9016 - accuracy: 0.7225 - val_loss: 0.5202 - val_accuracy: 0.8711\n",
      "Epoch 3/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.6858 - accuracy: 0.7891 - val_loss: 0.4189 - val_accuracy: 0.8903\n",
      "Epoch 4/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5822 - accuracy: 0.8221 - val_loss: 0.3655 - val_accuracy: 0.9003\n",
      "Epoch 5/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5245 - accuracy: 0.8438 - val_loss: 0.3332 - val_accuracy: 0.9072\n",
      "Epoch 6/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4824 - accuracy: 0.8548 - val_loss: 0.3110 - val_accuracy: 0.9118\n",
      "Epoch 7/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4540 - accuracy: 0.8645 - val_loss: 0.2924 - val_accuracy: 0.9165\n",
      "Epoch 8/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4273 - accuracy: 0.8755 - val_loss: 0.2767 - val_accuracy: 0.9205\n",
      "Epoch 9/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4058 - accuracy: 0.8804 - val_loss: 0.2651 - val_accuracy: 0.9225\n",
      "Epoch 10/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3857 - accuracy: 0.8863 - val_loss: 0.2533 - val_accuracy: 0.9264\n",
      "Epoch 11/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3715 - accuracy: 0.8904 - val_loss: 0.2442 - val_accuracy: 0.9285\n",
      "Epoch 12/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3557 - accuracy: 0.8934 - val_loss: 0.2347 - val_accuracy: 0.9307\n",
      "Epoch 13/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3433 - accuracy: 0.8999 - val_loss: 0.2270 - val_accuracy: 0.9331\n",
      "Epoch 14/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3303 - accuracy: 0.9026 - val_loss: 0.2200 - val_accuracy: 0.9351\n",
      "Epoch 15/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3215 - accuracy: 0.9053 - val_loss: 0.2136 - val_accuracy: 0.9367\n",
      "Epoch 16/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3141 - accuracy: 0.9075 - val_loss: 0.2076 - val_accuracy: 0.9392\n",
      "Epoch 17/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3039 - accuracy: 0.9102 - val_loss: 0.2018 - val_accuracy: 0.9407\n",
      "Epoch 18/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2951 - accuracy: 0.9131 - val_loss: 0.1967 - val_accuracy: 0.9423\n",
      "Epoch 19/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2894 - accuracy: 0.9136 - val_loss: 0.1916 - val_accuracy: 0.9438\n",
      "Epoch 20/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2831 - accuracy: 0.9170 - val_loss: 0.1875 - val_accuracy: 0.9456\n",
      "Epoch 21/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2775 - accuracy: 0.9181 - val_loss: 0.1838 - val_accuracy: 0.9465\n",
      "Epoch 22/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2708 - accuracy: 0.9194 - val_loss: 0.1804 - val_accuracy: 0.9487\n",
      "Epoch 23/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2653 - accuracy: 0.9214 - val_loss: 0.1760 - val_accuracy: 0.9502\n",
      "Epoch 24/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2609 - accuracy: 0.9227 - val_loss: 0.1719 - val_accuracy: 0.9507\n",
      "Epoch 25/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2538 - accuracy: 0.9257 - val_loss: 0.1686 - val_accuracy: 0.9509\n",
      "Epoch 26/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2494 - accuracy: 0.9285 - val_loss: 0.1653 - val_accuracy: 0.9523\n",
      "Epoch 27/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2453 - accuracy: 0.9274 - val_loss: 0.1633 - val_accuracy: 0.9528\n",
      "Epoch 28/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2407 - accuracy: 0.9281 - val_loss: 0.1605 - val_accuracy: 0.9540\n",
      "Epoch 29/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2343 - accuracy: 0.9305 - val_loss: 0.1580 - val_accuracy: 0.9549\n",
      "Epoch 30/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2305 - accuracy: 0.9321 - val_loss: 0.1552 - val_accuracy: 0.9548\n",
      "Epoch 31/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2302 - accuracy: 0.9332 - val_loss: 0.1536 - val_accuracy: 0.9559\n",
      "Epoch 32/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2254 - accuracy: 0.9345 - val_loss: 0.1503 - val_accuracy: 0.9572\n",
      "Epoch 33/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2223 - accuracy: 0.9339 - val_loss: 0.1488 - val_accuracy: 0.9577\n",
      "Epoch 34/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2150 - accuracy: 0.9376 - val_loss: 0.1464 - val_accuracy: 0.9581\n",
      "Epoch 35/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2169 - accuracy: 0.9360 - val_loss: 0.1456 - val_accuracy: 0.9581\n",
      "Epoch 36/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2122 - accuracy: 0.9369 - val_loss: 0.1425 - val_accuracy: 0.9596\n",
      "Epoch 37/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2060 - accuracy: 0.9394 - val_loss: 0.1401 - val_accuracy: 0.9602\n",
      "Epoch 38/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2046 - accuracy: 0.9394 - val_loss: 0.1393 - val_accuracy: 0.9602\n",
      "Epoch 39/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2008 - accuracy: 0.9405 - val_loss: 0.1378 - val_accuracy: 0.9602\n",
      "Epoch 40/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1996 - accuracy: 0.9414 - val_loss: 0.1356 - val_accuracy: 0.9609\n",
      "Epoch 41/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1979 - accuracy: 0.9418 - val_loss: 0.1342 - val_accuracy: 0.9607\n",
      "Epoch 42/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1952 - accuracy: 0.9414 - val_loss: 0.1326 - val_accuracy: 0.9621\n",
      "Epoch 43/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1916 - accuracy: 0.9436 - val_loss: 0.1317 - val_accuracy: 0.9625\n",
      "Epoch 44/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1887 - accuracy: 0.9431 - val_loss: 0.1303 - val_accuracy: 0.9617\n",
      "Epoch 45/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1857 - accuracy: 0.9447 - val_loss: 0.1294 - val_accuracy: 0.9625\n",
      "Epoch 46/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1856 - accuracy: 0.9456 - val_loss: 0.1273 - val_accuracy: 0.9634\n",
      "Epoch 47/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1838 - accuracy: 0.9452 - val_loss: 0.1267 - val_accuracy: 0.9632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1814 - accuracy: 0.9458 - val_loss: 0.1253 - val_accuracy: 0.9638\n",
      "Epoch 49/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1787 - accuracy: 0.9465 - val_loss: 0.1244 - val_accuracy: 0.9643\n",
      "Epoch 50/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1796 - accuracy: 0.9470 - val_loss: 0.1237 - val_accuracy: 0.9643\n",
      "Epoch 51/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1767 - accuracy: 0.9485 - val_loss: 0.1219 - val_accuracy: 0.9648\n",
      "Epoch 52/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1726 - accuracy: 0.9488 - val_loss: 0.1215 - val_accuracy: 0.9644\n",
      "Epoch 53/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1719 - accuracy: 0.9503 - val_loss: 0.1201 - val_accuracy: 0.9647\n",
      "Epoch 54/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1700 - accuracy: 0.9491 - val_loss: 0.1187 - val_accuracy: 0.9663\n",
      "Epoch 55/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1688 - accuracy: 0.9508 - val_loss: 0.1184 - val_accuracy: 0.9651\n",
      "Epoch 56/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1665 - accuracy: 0.9502 - val_loss: 0.1174 - val_accuracy: 0.9656\n",
      "Epoch 57/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1644 - accuracy: 0.9516 - val_loss: 0.1164 - val_accuracy: 0.9655\n",
      "Epoch 58/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1623 - accuracy: 0.9510 - val_loss: 0.1160 - val_accuracy: 0.9656\n",
      "Epoch 59/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1659 - accuracy: 0.9504 - val_loss: 0.1146 - val_accuracy: 0.9662\n",
      "Epoch 60/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1614 - accuracy: 0.9526 - val_loss: 0.1133 - val_accuracy: 0.9666\n",
      "Epoch 61/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1561 - accuracy: 0.9535 - val_loss: 0.1136 - val_accuracy: 0.9665\n",
      "Epoch 62/200\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1595 - accuracy: 0.95 - 1s 3ms/step - loss: 0.1587 - accuracy: 0.9524 - val_loss: 0.1130 - val_accuracy: 0.9673\n",
      "Epoch 63/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1576 - accuracy: 0.9518 - val_loss: 0.1121 - val_accuracy: 0.9671\n",
      "Epoch 64/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1553 - accuracy: 0.9544 - val_loss: 0.1116 - val_accuracy: 0.9672\n",
      "Epoch 65/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1536 - accuracy: 0.9540 - val_loss: 0.1101 - val_accuracy: 0.9673\n",
      "Epoch 66/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1530 - accuracy: 0.9539 - val_loss: 0.1095 - val_accuracy: 0.9678\n",
      "Epoch 67/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1509 - accuracy: 0.9563 - val_loss: 0.1094 - val_accuracy: 0.9678\n",
      "Epoch 68/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1506 - accuracy: 0.9554 - val_loss: 0.1098 - val_accuracy: 0.9675\n",
      "Epoch 69/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1483 - accuracy: 0.9556 - val_loss: 0.1084 - val_accuracy: 0.9684\n",
      "Epoch 70/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1498 - accuracy: 0.9563 - val_loss: 0.1070 - val_accuracy: 0.9686\n",
      "Epoch 71/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1464 - accuracy: 0.9569 - val_loss: 0.1075 - val_accuracy: 0.9688\n",
      "Epoch 72/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1444 - accuracy: 0.9566 - val_loss: 0.1060 - val_accuracy: 0.9689\n",
      "Epoch 73/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1468 - accuracy: 0.9557 - val_loss: 0.1056 - val_accuracy: 0.9690\n",
      "Epoch 74/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1453 - accuracy: 0.9563 - val_loss: 0.1055 - val_accuracy: 0.9688\n",
      "Epoch 75/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1418 - accuracy: 0.9582 - val_loss: 0.1051 - val_accuracy: 0.9693\n",
      "Epoch 76/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1399 - accuracy: 0.9588 - val_loss: 0.1042 - val_accuracy: 0.9693\n",
      "Epoch 77/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1393 - accuracy: 0.9582 - val_loss: 0.1031 - val_accuracy: 0.9697\n",
      "Epoch 78/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1403 - accuracy: 0.9578 - val_loss: 0.1034 - val_accuracy: 0.9690\n",
      "Epoch 79/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1378 - accuracy: 0.9590 - val_loss: 0.1024 - val_accuracy: 0.9703\n",
      "Epoch 80/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1371 - accuracy: 0.9581 - val_loss: 0.1021 - val_accuracy: 0.9704\n",
      "Epoch 81/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1357 - accuracy: 0.9592 - val_loss: 0.1014 - val_accuracy: 0.9703\n",
      "Epoch 82/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1372 - accuracy: 0.9589 - val_loss: 0.1012 - val_accuracy: 0.9702\n",
      "Epoch 83/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1327 - accuracy: 0.9608 - val_loss: 0.1007 - val_accuracy: 0.9707\n",
      "Epoch 84/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1335 - accuracy: 0.9599 - val_loss: 0.1000 - val_accuracy: 0.9705\n",
      "Epoch 85/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1315 - accuracy: 0.9614 - val_loss: 0.1000 - val_accuracy: 0.9712\n",
      "Epoch 86/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1306 - accuracy: 0.9608 - val_loss: 0.0995 - val_accuracy: 0.9703\n",
      "Epoch 87/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1283 - accuracy: 0.9613 - val_loss: 0.0990 - val_accuracy: 0.9708\n",
      "Epoch 88/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1304 - accuracy: 0.9618 - val_loss: 0.0980 - val_accuracy: 0.9711\n",
      "Epoch 89/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1296 - accuracy: 0.9616 - val_loss: 0.0986 - val_accuracy: 0.9705\n",
      "Epoch 90/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1257 - accuracy: 0.9625 - val_loss: 0.0978 - val_accuracy: 0.9710\n",
      "Epoch 91/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1268 - accuracy: 0.9620 - val_loss: 0.0970 - val_accuracy: 0.9716\n",
      "Epoch 92/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1248 - accuracy: 0.9631 - val_loss: 0.0970 - val_accuracy: 0.9714\n",
      "Epoch 93/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1264 - accuracy: 0.9626 - val_loss: 0.0967 - val_accuracy: 0.9717\n",
      "Epoch 94/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1239 - accuracy: 0.9634 - val_loss: 0.0964 - val_accuracy: 0.9712\n",
      "Epoch 95/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1230 - accuracy: 0.9633 - val_loss: 0.0952 - val_accuracy: 0.9716\n",
      "Epoch 96/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1218 - accuracy: 0.9634 - val_loss: 0.0955 - val_accuracy: 0.9717\n",
      "Epoch 97/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1213 - accuracy: 0.9634 - val_loss: 0.0952 - val_accuracy: 0.9718\n",
      "Epoch 98/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1240 - accuracy: 0.9632 - val_loss: 0.0945 - val_accuracy: 0.9713\n",
      "Epoch 99/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1211 - accuracy: 0.9637 - val_loss: 0.0945 - val_accuracy: 0.9719\n",
      "Epoch 100/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1192 - accuracy: 0.9645 - val_loss: 0.0943 - val_accuracy: 0.9722\n",
      "Epoch 101/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1198 - accuracy: 0.9639 - val_loss: 0.0939 - val_accuracy: 0.9718\n",
      "Epoch 102/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1183 - accuracy: 0.9644 - val_loss: 0.0936 - val_accuracy: 0.9723\n",
      "Epoch 103/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1159 - accuracy: 0.9659 - val_loss: 0.0929 - val_accuracy: 0.9728\n",
      "Epoch 104/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1162 - accuracy: 0.9651 - val_loss: 0.0928 - val_accuracy: 0.9727\n",
      "Epoch 105/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1153 - accuracy: 0.9658 - val_loss: 0.0925 - val_accuracy: 0.9723\n",
      "Epoch 106/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1158 - accuracy: 0.9650 - val_loss: 0.0925 - val_accuracy: 0.9728\n",
      "Epoch 107/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1136 - accuracy: 0.9656 - val_loss: 0.0921 - val_accuracy: 0.9728\n",
      "Epoch 108/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1139 - accuracy: 0.9655 - val_loss: 0.0916 - val_accuracy: 0.9729\n",
      "Epoch 109/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1121 - accuracy: 0.9662 - val_loss: 0.0913 - val_accuracy: 0.9727\n",
      "Epoch 110/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1130 - accuracy: 0.9668 - val_loss: 0.0914 - val_accuracy: 0.9733\n",
      "Epoch 111/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1124 - accuracy: 0.9659 - val_loss: 0.0910 - val_accuracy: 0.9729\n",
      "Epoch 112/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1118 - accuracy: 0.9659 - val_loss: 0.0904 - val_accuracy: 0.9733\n",
      "Epoch 113/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1095 - accuracy: 0.9670 - val_loss: 0.0901 - val_accuracy: 0.9732\n",
      "Epoch 114/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1090 - accuracy: 0.9665 - val_loss: 0.0904 - val_accuracy: 0.9737\n",
      "Epoch 115/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1080 - accuracy: 0.9674 - val_loss: 0.0900 - val_accuracy: 0.9732\n",
      "Epoch 116/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1075 - accuracy: 0.9684 - val_loss: 0.0897 - val_accuracy: 0.9733\n",
      "Epoch 117/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1113 - accuracy: 0.9657 - val_loss: 0.0894 - val_accuracy: 0.9736\n",
      "Epoch 118/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1045 - accuracy: 0.9691 - val_loss: 0.0890 - val_accuracy: 0.9738\n",
      "Epoch 119/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1052 - accuracy: 0.9683 - val_loss: 0.0891 - val_accuracy: 0.9737\n",
      "Epoch 120/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1056 - accuracy: 0.9682 - val_loss: 0.0890 - val_accuracy: 0.9732\n",
      "Epoch 121/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1031 - accuracy: 0.9696 - val_loss: 0.0887 - val_accuracy: 0.9737\n",
      "Epoch 122/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1069 - accuracy: 0.9677 - val_loss: 0.0882 - val_accuracy: 0.9738\n",
      "Epoch 123/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1023 - accuracy: 0.9691 - val_loss: 0.0886 - val_accuracy: 0.9741\n",
      "Epoch 124/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1047 - accuracy: 0.9683 - val_loss: 0.0876 - val_accuracy: 0.9737\n",
      "Epoch 125/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1020 - accuracy: 0.9688 - val_loss: 0.0882 - val_accuracy: 0.9735\n",
      "Epoch 126/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1043 - accuracy: 0.9682 - val_loss: 0.0880 - val_accuracy: 0.9737\n",
      "Epoch 127/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1033 - accuracy: 0.9683 - val_loss: 0.0874 - val_accuracy: 0.9736\n",
      "Epoch 128/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1018 - accuracy: 0.9694 - val_loss: 0.0875 - val_accuracy: 0.9737\n",
      "Epoch 129/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1006 - accuracy: 0.9691 - val_loss: 0.0871 - val_accuracy: 0.9739\n",
      "Epoch 130/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1003 - accuracy: 0.9700 - val_loss: 0.0869 - val_accuracy: 0.9745\n",
      "Epoch 131/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1015 - accuracy: 0.9695 - val_loss: 0.0872 - val_accuracy: 0.9735\n",
      "Epoch 132/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1009 - accuracy: 0.9693 - val_loss: 0.0869 - val_accuracy: 0.9737\n",
      "Epoch 133/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0987 - accuracy: 0.9697 - val_loss: 0.0864 - val_accuracy: 0.9736\n",
      "Epoch 134/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0984 - accuracy: 0.9707 - val_loss: 0.0865 - val_accuracy: 0.9742\n",
      "Epoch 135/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0966 - accuracy: 0.9704 - val_loss: 0.0865 - val_accuracy: 0.9743\n",
      "Epoch 136/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0974 - accuracy: 0.9700 - val_loss: 0.0861 - val_accuracy: 0.9744\n",
      "Epoch 137/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0979 - accuracy: 0.9700 - val_loss: 0.0860 - val_accuracy: 0.9748\n",
      "Epoch 138/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0945 - accuracy: 0.9714 - val_loss: 0.0856 - val_accuracy: 0.9745\n",
      "Epoch 139/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0976 - accuracy: 0.9702 - val_loss: 0.0860 - val_accuracy: 0.9747\n",
      "Epoch 140/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0963 - accuracy: 0.9711 - val_loss: 0.0849 - val_accuracy: 0.9747\n",
      "Epoch 141/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0930 - accuracy: 0.9711 - val_loss: 0.0850 - val_accuracy: 0.9747\n",
      "Epoch 142/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0941 - accuracy: 0.9710 - val_loss: 0.0853 - val_accuracy: 0.9749\n",
      "Epoch 143/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0955 - accuracy: 0.9714 - val_loss: 0.0853 - val_accuracy: 0.9747\n",
      "Epoch 144/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0943 - accuracy: 0.9716 - val_loss: 0.0849 - val_accuracy: 0.9753\n",
      "Epoch 145/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0967 - accuracy: 0.9705 - val_loss: 0.0850 - val_accuracy: 0.9745\n",
      "Epoch 146/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0903 - accuracy: 0.9714 - val_loss: 0.0849 - val_accuracy: 0.9747\n",
      "Epoch 147/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0924 - accuracy: 0.9707 - val_loss: 0.0846 - val_accuracy: 0.9754\n",
      "Epoch 148/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0908 - accuracy: 0.9720 - val_loss: 0.0844 - val_accuracy: 0.9743\n",
      "Epoch 149/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0931 - accuracy: 0.9717 - val_loss: 0.0848 - val_accuracy: 0.9747\n",
      "Epoch 150/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0922 - accuracy: 0.9720 - val_loss: 0.0844 - val_accuracy: 0.9753\n",
      "Epoch 151/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0899 - accuracy: 0.9727 - val_loss: 0.0842 - val_accuracy: 0.9747\n",
      "Epoch 152/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0892 - accuracy: 0.9726 - val_loss: 0.0843 - val_accuracy: 0.9754\n",
      "Epoch 153/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0927 - accuracy: 0.9713 - val_loss: 0.0840 - val_accuracy: 0.9752\n",
      "Epoch 154/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0916 - accuracy: 0.9727 - val_loss: 0.0837 - val_accuracy: 0.9753\n",
      "Epoch 155/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0891 - accuracy: 0.9728 - val_loss: 0.0841 - val_accuracy: 0.9747\n",
      "Epoch 156/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0912 - accuracy: 0.9723 - val_loss: 0.0837 - val_accuracy: 0.9753\n",
      "Epoch 157/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0875 - accuracy: 0.9728 - val_loss: 0.0839 - val_accuracy: 0.9749\n",
      "Epoch 158/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0899 - accuracy: 0.9728 - val_loss: 0.0830 - val_accuracy: 0.9753\n",
      "Epoch 159/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0893 - accuracy: 0.9725 - val_loss: 0.0832 - val_accuracy: 0.9751\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0885 - accuracy: 0.9732 - val_loss: 0.0825 - val_accuracy: 0.9758\n",
      "Epoch 161/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0869 - accuracy: 0.9730 - val_loss: 0.0825 - val_accuracy: 0.9750\n",
      "Epoch 162/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0881 - accuracy: 0.9737 - val_loss: 0.0831 - val_accuracy: 0.9753\n",
      "Epoch 163/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0862 - accuracy: 0.9736 - val_loss: 0.0826 - val_accuracy: 0.9755\n",
      "Epoch 164/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0871 - accuracy: 0.9737 - val_loss: 0.0821 - val_accuracy: 0.9757\n",
      "Epoch 165/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0865 - accuracy: 0.9733 - val_loss: 0.0820 - val_accuracy: 0.9753\n",
      "Epoch 166/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0848 - accuracy: 0.9739 - val_loss: 0.0820 - val_accuracy: 0.9753\n",
      "Epoch 167/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0851 - accuracy: 0.9741 - val_loss: 0.0823 - val_accuracy: 0.9750\n",
      "Epoch 168/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0855 - accuracy: 0.9739 - val_loss: 0.0817 - val_accuracy: 0.9753\n",
      "Epoch 169/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0836 - accuracy: 0.9746 - val_loss: 0.0818 - val_accuracy: 0.9755\n",
      "Epoch 170/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0853 - accuracy: 0.9740 - val_loss: 0.0817 - val_accuracy: 0.9754\n",
      "Epoch 171/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0844 - accuracy: 0.9740 - val_loss: 0.0816 - val_accuracy: 0.9759\n",
      "Epoch 172/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0837 - accuracy: 0.9740 - val_loss: 0.0815 - val_accuracy: 0.9755\n",
      "Epoch 173/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0847 - accuracy: 0.9736 - val_loss: 0.0811 - val_accuracy: 0.9758\n",
      "Epoch 174/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0821 - accuracy: 0.9742 - val_loss: 0.0811 - val_accuracy: 0.9756\n",
      "Epoch 175/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0829 - accuracy: 0.9744 - val_loss: 0.0814 - val_accuracy: 0.9754\n",
      "Epoch 176/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0818 - accuracy: 0.9747 - val_loss: 0.0814 - val_accuracy: 0.9753\n",
      "Epoch 177/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0825 - accuracy: 0.9747 - val_loss: 0.0812 - val_accuracy: 0.9757\n",
      "Epoch 178/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0826 - accuracy: 0.9745 - val_loss: 0.0817 - val_accuracy: 0.9753\n",
      "Epoch 179/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0816 - accuracy: 0.9749 - val_loss: 0.0810 - val_accuracy: 0.9758\n",
      "Epoch 180/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0789 - accuracy: 0.9760 - val_loss: 0.0813 - val_accuracy: 0.9753\n",
      "Epoch 181/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0815 - accuracy: 0.9748 - val_loss: 0.0811 - val_accuracy: 0.9756\n",
      "Epoch 182/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0794 - accuracy: 0.9749 - val_loss: 0.0808 - val_accuracy: 0.9759\n",
      "Epoch 183/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0793 - accuracy: 0.9749 - val_loss: 0.0801 - val_accuracy: 0.9758\n",
      "Epoch 184/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0797 - accuracy: 0.9761 - val_loss: 0.0806 - val_accuracy: 0.9758\n",
      "Epoch 185/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0781 - accuracy: 0.9775 - val_loss: 0.0809 - val_accuracy: 0.9757\n",
      "Epoch 186/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0792 - accuracy: 0.9758 - val_loss: 0.0801 - val_accuracy: 0.9757\n",
      "Epoch 187/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0777 - accuracy: 0.9758 - val_loss: 0.0799 - val_accuracy: 0.9759\n",
      "Epoch 188/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0780 - accuracy: 0.9757 - val_loss: 0.0799 - val_accuracy: 0.9760\n",
      "Epoch 189/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0788 - accuracy: 0.9751 - val_loss: 0.0796 - val_accuracy: 0.9762\n",
      "Epoch 190/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0775 - accuracy: 0.9760 - val_loss: 0.0795 - val_accuracy: 0.9762\n",
      "Epoch 191/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0766 - accuracy: 0.9766 - val_loss: 0.0800 - val_accuracy: 0.9759\n",
      "Epoch 192/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0775 - accuracy: 0.9757 - val_loss: 0.0799 - val_accuracy: 0.9758\n",
      "Epoch 193/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0773 - accuracy: 0.9759 - val_loss: 0.0799 - val_accuracy: 0.9759\n",
      "Epoch 194/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0763 - accuracy: 0.9770 - val_loss: 0.0804 - val_accuracy: 0.9761\n",
      "Epoch 195/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0763 - accuracy: 0.9760 - val_loss: 0.0796 - val_accuracy: 0.9761\n",
      "Epoch 196/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0754 - accuracy: 0.9768 - val_loss: 0.0796 - val_accuracy: 0.9760\n",
      "Epoch 197/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0775 - accuracy: 0.9758 - val_loss: 0.0792 - val_accuracy: 0.9765\n",
      "Epoch 198/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0726 - accuracy: 0.9776 - val_loss: 0.0792 - val_accuracy: 0.9760\n",
      "Epoch 199/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0745 - accuracy: 0.9768 - val_loss: 0.0788 - val_accuracy: 0.9764\n",
      "Epoch 200/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0751 - accuracy: 0.9772 - val_loss: 0.0795 - val_accuracy: 0.9760\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 0.0743 - accuracy: 0.9775\n",
      "\n",
      "Test score: 0.07434017211198807\n",
      "Test accuracy: 0.9775000214576721\n",
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import dos pacotes\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Garantindo que o resultado pode ser reproduzido\n",
    "np.random.seed(1671)  \n",
    "\n",
    "# Parâmetros da rede e do treinamento\n",
    "NB_EPOCH = 200\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "VERBOSE = 1\n",
    "\n",
    "NB_CLASSES = 10   # número de outputs = número de dígitos\n",
    "\n",
    "OPTIMIZER = tensorflow.keras.optimizers.SGD() #Otimizador SGD\n",
    "\n",
    "N_HIDDEN = 128\n",
    "\n",
    "VALIDATION_SPLIT = 0.2 # quanto é reservado para validação\n",
    "\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# Gerando datasets de treino e e teste\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# X_train possui 60000 linhas de valores 28x28 --> reshape para 60000 x 784\n",
    "# Gera versão final dos datasetes de treino e de teste\n",
    "RESHAPED = 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalizando os dados\n",
    "# Tipicamente, os valores associados a cada pixel são normalizados na faixa [0, 1] \n",
    "# (o que significa que a intensidade de cada pixel é dividida por 255, o valor de intensidade máxima). \n",
    "# A saída é 10 classes, uma para cada dígito.\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'exemplos de treino')\n",
    "print(X_test.shape[0], 'exemplos de teste')\n",
    "\n",
    "# Converte os vetores da class para matrizes binárias das classes\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# Cria as camadas\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Sumário\n",
    "model.summary()\n",
    "\n",
    "# Compila o modelo\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = OPTIMIZER, metrics = ['accuracy'])\n",
    "\n",
    "# Treina o modelo\n",
    "modelo_v3 = model.fit(X_train, Y_train,\n",
    "                      batch_size = BATCH_SIZE, \n",
    "                      epochs = NB_EPOCH,\n",
    "                      verbose = VERBOSE, \n",
    "                      validation_split = VALIDATION_SPLIT)\n",
    "\n",
    "# Avalia o modelo com os dados de teste\n",
    "score = model.evaluate(X_test, Y_test, verbose = VERBOSE)\n",
    "\n",
    "# Imprime a perda e a acurácia\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Imprime os dados\n",
    "print(modelo_v3.history.keys())\n",
    "\n",
    "# Sumariza o modelo para acurácia\n",
    "plt.plot(modelo_v3.history['accuracy'])\n",
    "plt.plot(modelo_v3.history['val_accuracy'])\n",
    "plt.title('Acurácia do Modelo')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Treino', 'Teste'], loc = 'upper left')\n",
    "plt.show()\n",
    "\n",
    "# Imprime a evolução de erro do modelo\n",
    "plt.plot(modelo_v3.history['loss'])\n",
    "plt.plot(modelo_v3.history['val_loss'])\n",
    "plt.title('Perda do Modelo')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Treino', 'Teste'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.6924406290054321,\n",
       "  0.9015756845474243,\n",
       "  0.6858315467834473,\n",
       "  0.5822331309318542,\n",
       "  0.5245140790939331,\n",
       "  0.48242077231407166,\n",
       "  0.4539816379547119,\n",
       "  0.42727163434028625,\n",
       "  0.4058328866958618,\n",
       "  0.3857342004776001,\n",
       "  0.37150129675865173,\n",
       "  0.3556758761405945,\n",
       "  0.3432624936103821,\n",
       "  0.3303335905075073,\n",
       "  0.3214556872844696,\n",
       "  0.3141126334667206,\n",
       "  0.30394038558006287,\n",
       "  0.2950921952724457,\n",
       "  0.28941860795021057,\n",
       "  0.2831115424633026,\n",
       "  0.2775408625602722,\n",
       "  0.2708139717578888,\n",
       "  0.2653414011001587,\n",
       "  0.2609456479549408,\n",
       "  0.25375035405158997,\n",
       "  0.24940960109233856,\n",
       "  0.24532915651798248,\n",
       "  0.24071380496025085,\n",
       "  0.2343137264251709,\n",
       "  0.23054465651512146,\n",
       "  0.23024731874465942,\n",
       "  0.2254115343093872,\n",
       "  0.22231538593769073,\n",
       "  0.21503637731075287,\n",
       "  0.21685785055160522,\n",
       "  0.21218721568584442,\n",
       "  0.20602908730506897,\n",
       "  0.20459452271461487,\n",
       "  0.20078741014003754,\n",
       "  0.19962754845619202,\n",
       "  0.1979237049818039,\n",
       "  0.19523419439792633,\n",
       "  0.19163624942302704,\n",
       "  0.18871213495731354,\n",
       "  0.1856825351715088,\n",
       "  0.1855534464120865,\n",
       "  0.18380992114543915,\n",
       "  0.18135015666484833,\n",
       "  0.17866434156894684,\n",
       "  0.1796332746744156,\n",
       "  0.17671999335289001,\n",
       "  0.17257057130336761,\n",
       "  0.17189592123031616,\n",
       "  0.16996516287326813,\n",
       "  0.16878332197666168,\n",
       "  0.16651029884815216,\n",
       "  0.16439932584762573,\n",
       "  0.16233326494693756,\n",
       "  0.16590139269828796,\n",
       "  0.1614484339952469,\n",
       "  0.15612764656543732,\n",
       "  0.1587364673614502,\n",
       "  0.1576152890920639,\n",
       "  0.15531449019908905,\n",
       "  0.15362370014190674,\n",
       "  0.1530138999223709,\n",
       "  0.15088774263858795,\n",
       "  0.15063880383968353,\n",
       "  0.14827995002269745,\n",
       "  0.1497650295495987,\n",
       "  0.14638744294643402,\n",
       "  0.14439675211906433,\n",
       "  0.1467544138431549,\n",
       "  0.14527074992656708,\n",
       "  0.14180831611156464,\n",
       "  0.13990852236747742,\n",
       "  0.1392982006072998,\n",
       "  0.1402565985918045,\n",
       "  0.1378011852502823,\n",
       "  0.13705161213874817,\n",
       "  0.13566330075263977,\n",
       "  0.13720563054084778,\n",
       "  0.1326642483472824,\n",
       "  0.13347645103931427,\n",
       "  0.13146115839481354,\n",
       "  0.13060563802719116,\n",
       "  0.1282733976840973,\n",
       "  0.13039487600326538,\n",
       "  0.12959182262420654,\n",
       "  0.12565353512763977,\n",
       "  0.1267535388469696,\n",
       "  0.12475555390119553,\n",
       "  0.12643830478191376,\n",
       "  0.12390914559364319,\n",
       "  0.1230430006980896,\n",
       "  0.12180155515670776,\n",
       "  0.12134548276662827,\n",
       "  0.12398874014616013,\n",
       "  0.12108590453863144,\n",
       "  0.1192406713962555,\n",
       "  0.11980415135622025,\n",
       "  0.11833600699901581,\n",
       "  0.11587356775999069,\n",
       "  0.11624158918857574,\n",
       "  0.11534983664751053,\n",
       "  0.11577458679676056,\n",
       "  0.1136142760515213,\n",
       "  0.11389827728271484,\n",
       "  0.11212687939405441,\n",
       "  0.1129508689045906,\n",
       "  0.11244798451662064,\n",
       "  0.11175798624753952,\n",
       "  0.10948267579078674,\n",
       "  0.10896791517734528,\n",
       "  0.10802987962961197,\n",
       "  0.10752879083156586,\n",
       "  0.11129286885261536,\n",
       "  0.10453862696886063,\n",
       "  0.10517410933971405,\n",
       "  0.1055678278207779,\n",
       "  0.10308526456356049,\n",
       "  0.10694657266139984,\n",
       "  0.10234417766332626,\n",
       "  0.10470151901245117,\n",
       "  0.1020360141992569,\n",
       "  0.10426995158195496,\n",
       "  0.10332124680280685,\n",
       "  0.10179004073143005,\n",
       "  0.10063530504703522,\n",
       "  0.10029376298189163,\n",
       "  0.10148472338914871,\n",
       "  0.10091596841812134,\n",
       "  0.09874151647090912,\n",
       "  0.09841329604387283,\n",
       "  0.09661194682121277,\n",
       "  0.09735293686389923,\n",
       "  0.09786411374807358,\n",
       "  0.09448320418596268,\n",
       "  0.09760479629039764,\n",
       "  0.0962933674454689,\n",
       "  0.09298837184906006,\n",
       "  0.09414015710353851,\n",
       "  0.0955030620098114,\n",
       "  0.09432194381952286,\n",
       "  0.09669122099876404,\n",
       "  0.09034916013479233,\n",
       "  0.09238635003566742,\n",
       "  0.0907912328839302,\n",
       "  0.09306209534406662,\n",
       "  0.09223306179046631,\n",
       "  0.08993066102266312,\n",
       "  0.08920831233263016,\n",
       "  0.09267259389162064,\n",
       "  0.09160414338111877,\n",
       "  0.08914828300476074,\n",
       "  0.09122428297996521,\n",
       "  0.08753518760204315,\n",
       "  0.0899224728345871,\n",
       "  0.08934345841407776,\n",
       "  0.08851224929094315,\n",
       "  0.08691105246543884,\n",
       "  0.0880580022931099,\n",
       "  0.08618979901075363,\n",
       "  0.087143175303936,\n",
       "  0.08649410307407379,\n",
       "  0.08479390293359756,\n",
       "  0.08509186655282974,\n",
       "  0.08549679815769196,\n",
       "  0.08362996578216553,\n",
       "  0.08534801006317139,\n",
       "  0.08437518030405045,\n",
       "  0.08366765081882477,\n",
       "  0.08465715497732162,\n",
       "  0.08212505280971527,\n",
       "  0.08286458998918533,\n",
       "  0.08181391656398773,\n",
       "  0.08253301680088043,\n",
       "  0.08258355408906937,\n",
       "  0.0816262736916542,\n",
       "  0.07889464497566223,\n",
       "  0.0815143808722496,\n",
       "  0.07942942529916763,\n",
       "  0.07928364723920822,\n",
       "  0.0796591192483902,\n",
       "  0.07809314131736755,\n",
       "  0.0791560634970665,\n",
       "  0.07770976424217224,\n",
       "  0.07802990078926086,\n",
       "  0.07882487773895264,\n",
       "  0.0774923786520958,\n",
       "  0.07656766474246979,\n",
       "  0.07748214155435562,\n",
       "  0.07727058231830597,\n",
       "  0.07626283168792725,\n",
       "  0.07626006752252579,\n",
       "  0.07538161426782608,\n",
       "  0.07748176902532578,\n",
       "  0.07264338433742523,\n",
       "  0.07450367510318756,\n",
       "  0.07510224729776382],\n",
       " 'accuracy': [0.468625009059906,\n",
       "  0.7225416898727417,\n",
       "  0.789145827293396,\n",
       "  0.8221458196640015,\n",
       "  0.843791663646698,\n",
       "  0.854812502861023,\n",
       "  0.8645208477973938,\n",
       "  0.8755208253860474,\n",
       "  0.8803750276565552,\n",
       "  0.8863124847412109,\n",
       "  0.89041668176651,\n",
       "  0.8934375047683716,\n",
       "  0.8998958468437195,\n",
       "  0.9025624990463257,\n",
       "  0.9053333401679993,\n",
       "  0.9075208306312561,\n",
       "  0.9101874828338623,\n",
       "  0.9130833148956299,\n",
       "  0.9135624766349792,\n",
       "  0.917020857334137,\n",
       "  0.9180833101272583,\n",
       "  0.9193541407585144,\n",
       "  0.9214375019073486,\n",
       "  0.9226666688919067,\n",
       "  0.9257291555404663,\n",
       "  0.9284791946411133,\n",
       "  0.9273750185966492,\n",
       "  0.9281250238418579,\n",
       "  0.9304583072662354,\n",
       "  0.9321458339691162,\n",
       "  0.9331666827201843,\n",
       "  0.934458315372467,\n",
       "  0.9338958263397217,\n",
       "  0.9375625252723694,\n",
       "  0.9359791874885559,\n",
       "  0.9368958473205566,\n",
       "  0.9393958449363708,\n",
       "  0.9393541812896729,\n",
       "  0.9404791593551636,\n",
       "  0.9413958191871643,\n",
       "  0.9418125152587891,\n",
       "  0.9413750171661377,\n",
       "  0.9436041712760925,\n",
       "  0.9431041479110718,\n",
       "  0.9446666836738586,\n",
       "  0.945562481880188,\n",
       "  0.945229172706604,\n",
       "  0.9458125233650208,\n",
       "  0.9465416669845581,\n",
       "  0.9470208287239075,\n",
       "  0.948479175567627,\n",
       "  0.9487500190734863,\n",
       "  0.9503124952316284,\n",
       "  0.9491041898727417,\n",
       "  0.9507916569709778,\n",
       "  0.9501875042915344,\n",
       "  0.9515833258628845,\n",
       "  0.9510416388511658,\n",
       "  0.9503541588783264,\n",
       "  0.9526041746139526,\n",
       "  0.953458309173584,\n",
       "  0.9523749947547913,\n",
       "  0.9517916440963745,\n",
       "  0.9543750286102295,\n",
       "  0.9539791941642761,\n",
       "  0.9539166688919067,\n",
       "  0.956250011920929,\n",
       "  0.9554166793823242,\n",
       "  0.9555624723434448,\n",
       "  0.9562708139419556,\n",
       "  0.9569166898727417,\n",
       "  0.9565625190734863,\n",
       "  0.9556875228881836,\n",
       "  0.9563124775886536,\n",
       "  0.9581666588783264,\n",
       "  0.9587500095367432,\n",
       "  0.9581875205039978,\n",
       "  0.957770824432373,\n",
       "  0.9590208530426025,\n",
       "  0.9581041932106018,\n",
       "  0.9592083096504211,\n",
       "  0.9589375257492065,\n",
       "  0.9608333110809326,\n",
       "  0.9598541855812073,\n",
       "  0.9613958597183228,\n",
       "  0.9607916474342346,\n",
       "  0.961270809173584,\n",
       "  0.9617916941642761,\n",
       "  0.9615625143051147,\n",
       "  0.9624583125114441,\n",
       "  0.9620000123977661,\n",
       "  0.9630833268165588,\n",
       "  0.9626041650772095,\n",
       "  0.9634374976158142,\n",
       "  0.9633125066757202,\n",
       "  0.9634374976158142,\n",
       "  0.9634166955947876,\n",
       "  0.9632291793823242,\n",
       "  0.9637083411216736,\n",
       "  0.9645208120346069,\n",
       "  0.963937520980835,\n",
       "  0.9644166827201843,\n",
       "  0.9658541679382324,\n",
       "  0.9651041626930237,\n",
       "  0.965833306312561,\n",
       "  0.9650208353996277,\n",
       "  0.965624988079071,\n",
       "  0.965541660785675,\n",
       "  0.9662083387374878,\n",
       "  0.9667916893959045,\n",
       "  0.9659374952316284,\n",
       "  0.9658541679382324,\n",
       "  0.9670208096504211,\n",
       "  0.9665208458900452,\n",
       "  0.9674375057220459,\n",
       "  0.968416690826416,\n",
       "  0.9657291769981384,\n",
       "  0.9690625071525574,\n",
       "  0.9683333039283752,\n",
       "  0.9682291746139526,\n",
       "  0.9695833325386047,\n",
       "  0.9676874876022339,\n",
       "  0.9691041707992554,\n",
       "  0.9683333039283752,\n",
       "  0.968791663646698,\n",
       "  0.9681875109672546,\n",
       "  0.9682708382606506,\n",
       "  0.9693541526794434,\n",
       "  0.969083309173584,\n",
       "  0.9700000286102295,\n",
       "  0.9694791436195374,\n",
       "  0.9692708253860474,\n",
       "  0.9697083234786987,\n",
       "  0.9707083106040955,\n",
       "  0.9703958630561829,\n",
       "  0.9700000286102295,\n",
       "  0.9699583053588867,\n",
       "  0.9714166522026062,\n",
       "  0.9702083468437195,\n",
       "  0.9710624814033508,\n",
       "  0.9710624814033508,\n",
       "  0.9710208177566528,\n",
       "  0.9714166522026062,\n",
       "  0.9715833067893982,\n",
       "  0.9704791903495789,\n",
       "  0.9714375138282776,\n",
       "  0.9706875085830688,\n",
       "  0.972000002861023,\n",
       "  0.971666693687439,\n",
       "  0.972041666507721,\n",
       "  0.9726874828338623,\n",
       "  0.9726250171661377,\n",
       "  0.9712916612625122,\n",
       "  0.9727083444595337,\n",
       "  0.9727500081062317,\n",
       "  0.9723125100135803,\n",
       "  0.9727916717529297,\n",
       "  0.9728124737739563,\n",
       "  0.9725416898727417,\n",
       "  0.9732499718666077,\n",
       "  0.9730208516120911,\n",
       "  0.9737499952316284,\n",
       "  0.9736250042915344,\n",
       "  0.9736666679382324,\n",
       "  0.973312497138977,\n",
       "  0.9739375114440918,\n",
       "  0.9741041660308838,\n",
       "  0.9738541841506958,\n",
       "  0.9746249914169312,\n",
       "  0.9739791750907898,\n",
       "  0.9739999771118164,\n",
       "  0.9739583134651184,\n",
       "  0.973562479019165,\n",
       "  0.9742291569709778,\n",
       "  0.9743750095367432,\n",
       "  0.9746875166893005,\n",
       "  0.9746666550636292,\n",
       "  0.9744791388511658,\n",
       "  0.9748541712760925,\n",
       "  0.9759583473205566,\n",
       "  0.9747916460037231,\n",
       "  0.9748749732971191,\n",
       "  0.9748958349227905,\n",
       "  0.9761458039283752,\n",
       "  0.9775208234786987,\n",
       "  0.9757916927337646,\n",
       "  0.9757708311080933,\n",
       "  0.9756666421890259,\n",
       "  0.9751250147819519,\n",
       "  0.9760000109672546,\n",
       "  0.9766250252723694,\n",
       "  0.9757291674613953,\n",
       "  0.9759166836738586,\n",
       "  0.9770416617393494,\n",
       "  0.9760000109672546,\n",
       "  0.9767916798591614,\n",
       "  0.9757708311080933,\n",
       "  0.9776458144187927,\n",
       "  0.9767500162124634,\n",
       "  0.9772499799728394],\n",
       " 'val_loss': [0.8728817701339722,\n",
       "  0.520244836807251,\n",
       "  0.4189232587814331,\n",
       "  0.3654831647872925,\n",
       "  0.3332442045211792,\n",
       "  0.3109871745109558,\n",
       "  0.2924182415008545,\n",
       "  0.27666205167770386,\n",
       "  0.26505208015441895,\n",
       "  0.25327828526496887,\n",
       "  0.24424152076244354,\n",
       "  0.2346908152103424,\n",
       "  0.22703205049037933,\n",
       "  0.21995426714420319,\n",
       "  0.2135627418756485,\n",
       "  0.20764780044555664,\n",
       "  0.20182038843631744,\n",
       "  0.19670815765857697,\n",
       "  0.19163712859153748,\n",
       "  0.1875312328338623,\n",
       "  0.18379488587379456,\n",
       "  0.1803531050682068,\n",
       "  0.17598319053649902,\n",
       "  0.17188292741775513,\n",
       "  0.16861240565776825,\n",
       "  0.1652914434671402,\n",
       "  0.1632910966873169,\n",
       "  0.16051584482192993,\n",
       "  0.15802538394927979,\n",
       "  0.15515552461147308,\n",
       "  0.15355148911476135,\n",
       "  0.15027788281440735,\n",
       "  0.14875799417495728,\n",
       "  0.14641527831554413,\n",
       "  0.14559079706668854,\n",
       "  0.14250408113002777,\n",
       "  0.14010989665985107,\n",
       "  0.13930808007717133,\n",
       "  0.13779765367507935,\n",
       "  0.13555669784545898,\n",
       "  0.13422565162181854,\n",
       "  0.13263653218746185,\n",
       "  0.13167111575603485,\n",
       "  0.1303086280822754,\n",
       "  0.12940055131912231,\n",
       "  0.12733694911003113,\n",
       "  0.1266738623380661,\n",
       "  0.12528447806835175,\n",
       "  0.1244078055024147,\n",
       "  0.12369685620069504,\n",
       "  0.12187807261943817,\n",
       "  0.12153872847557068,\n",
       "  0.12005170434713364,\n",
       "  0.11870672553777695,\n",
       "  0.11842703074216843,\n",
       "  0.11736314743757248,\n",
       "  0.11636416614055634,\n",
       "  0.11599304527044296,\n",
       "  0.11464276909828186,\n",
       "  0.11327556520700455,\n",
       "  0.11357639729976654,\n",
       "  0.11300066858530045,\n",
       "  0.11210040003061295,\n",
       "  0.11162608861923218,\n",
       "  0.11008509248495102,\n",
       "  0.10950857400894165,\n",
       "  0.10937322676181793,\n",
       "  0.109835185110569,\n",
       "  0.10840737074613571,\n",
       "  0.10697594285011292,\n",
       "  0.10751862823963165,\n",
       "  0.1060352474451065,\n",
       "  0.10561799257993698,\n",
       "  0.1054731160402298,\n",
       "  0.10514665395021439,\n",
       "  0.10417378693819046,\n",
       "  0.10313697159290314,\n",
       "  0.10337755084037781,\n",
       "  0.10239661484956741,\n",
       "  0.1020798310637474,\n",
       "  0.10139922052621841,\n",
       "  0.10117398947477341,\n",
       "  0.10066872090101242,\n",
       "  0.09997953474521637,\n",
       "  0.10000445693731308,\n",
       "  0.09945791959762573,\n",
       "  0.09897409379482269,\n",
       "  0.09798714518547058,\n",
       "  0.0985945537686348,\n",
       "  0.09779699891805649,\n",
       "  0.09699530899524689,\n",
       "  0.09703567624092102,\n",
       "  0.09673766791820526,\n",
       "  0.09638933092355728,\n",
       "  0.09518082439899445,\n",
       "  0.09552233666181564,\n",
       "  0.09516008198261261,\n",
       "  0.09448850154876709,\n",
       "  0.09454058855772018,\n",
       "  0.09428335726261139,\n",
       "  0.09388456493616104,\n",
       "  0.0936349406838417,\n",
       "  0.09291815012693405,\n",
       "  0.09279360622167587,\n",
       "  0.09251537919044495,\n",
       "  0.0925154909491539,\n",
       "  0.09206894040107727,\n",
       "  0.09156724065542221,\n",
       "  0.0912669450044632,\n",
       "  0.09136585146188736,\n",
       "  0.09096694737672806,\n",
       "  0.09044378995895386,\n",
       "  0.09012796729803085,\n",
       "  0.0903669148683548,\n",
       "  0.09001979231834412,\n",
       "  0.08967123180627823,\n",
       "  0.08941522240638733,\n",
       "  0.08899171650409698,\n",
       "  0.08906951546669006,\n",
       "  0.08898948132991791,\n",
       "  0.08865272998809814,\n",
       "  0.08815231174230576,\n",
       "  0.08863658457994461,\n",
       "  0.08761507272720337,\n",
       "  0.08821428567171097,\n",
       "  0.08796258270740509,\n",
       "  0.08740000426769257,\n",
       "  0.08748400956392288,\n",
       "  0.08705387264490128,\n",
       "  0.08692523092031479,\n",
       "  0.0872281864285469,\n",
       "  0.08686836063861847,\n",
       "  0.08641453832387924,\n",
       "  0.08654102683067322,\n",
       "  0.08645754307508469,\n",
       "  0.08612232655286789,\n",
       "  0.08596288412809372,\n",
       "  0.08563335239887238,\n",
       "  0.08598028868436813,\n",
       "  0.0848923921585083,\n",
       "  0.085027776658535,\n",
       "  0.08526582270860672,\n",
       "  0.08527044951915741,\n",
       "  0.0849439725279808,\n",
       "  0.08497492223978043,\n",
       "  0.084859699010849,\n",
       "  0.08459970355033875,\n",
       "  0.08435407280921936,\n",
       "  0.08478016406297684,\n",
       "  0.0843810886144638,\n",
       "  0.08417046815156937,\n",
       "  0.08427007496356964,\n",
       "  0.08400154113769531,\n",
       "  0.08374761044979095,\n",
       "  0.08408089727163315,\n",
       "  0.08368519693613052,\n",
       "  0.08387503027915955,\n",
       "  0.08304423838853836,\n",
       "  0.0831809714436531,\n",
       "  0.0824553593993187,\n",
       "  0.08245907723903656,\n",
       "  0.08306217193603516,\n",
       "  0.08257284760475159,\n",
       "  0.08211742341518402,\n",
       "  0.08203350007534027,\n",
       "  0.08198123425245285,\n",
       "  0.08229969441890717,\n",
       "  0.08171246200799942,\n",
       "  0.0817723348736763,\n",
       "  0.08172532916069031,\n",
       "  0.08157461881637573,\n",
       "  0.08145944774150848,\n",
       "  0.08107511699199677,\n",
       "  0.08113230764865875,\n",
       "  0.08136869966983795,\n",
       "  0.08136458694934845,\n",
       "  0.08115312457084656,\n",
       "  0.08170769363641739,\n",
       "  0.0810137689113617,\n",
       "  0.08132385462522507,\n",
       "  0.08106030523777008,\n",
       "  0.0808204784989357,\n",
       "  0.08010738343000412,\n",
       "  0.08055847883224487,\n",
       "  0.08088597655296326,\n",
       "  0.08007216453552246,\n",
       "  0.07985730469226837,\n",
       "  0.07991344481706619,\n",
       "  0.07961250096559525,\n",
       "  0.07954739034175873,\n",
       "  0.0799671933054924,\n",
       "  0.07991190254688263,\n",
       "  0.0798993706703186,\n",
       "  0.0804268941283226,\n",
       "  0.07957123965024948,\n",
       "  0.07964266091585159,\n",
       "  0.07920780777931213,\n",
       "  0.07922130078077316,\n",
       "  0.07879560440778732,\n",
       "  0.07948929071426392],\n",
       " 'val_accuracy': [0.8189166784286499,\n",
       "  0.8710833191871643,\n",
       "  0.890333354473114,\n",
       "  0.9002500176429749,\n",
       "  0.9071666598320007,\n",
       "  0.9117500185966492,\n",
       "  0.9164999723434448,\n",
       "  0.9204999804496765,\n",
       "  0.9225000143051147,\n",
       "  0.9264166951179504,\n",
       "  0.9284999966621399,\n",
       "  0.9306666851043701,\n",
       "  0.9330833554267883,\n",
       "  0.9350833296775818,\n",
       "  0.9366666674613953,\n",
       "  0.9392499923706055,\n",
       "  0.940666675567627,\n",
       "  0.9422500133514404,\n",
       "  0.9437500238418579,\n",
       "  0.9455833435058594,\n",
       "  0.9465000033378601,\n",
       "  0.9486666917800903,\n",
       "  0.950166642665863,\n",
       "  0.9507499933242798,\n",
       "  0.9509166479110718,\n",
       "  0.9523333311080933,\n",
       "  0.952833354473114,\n",
       "  0.9539999961853027,\n",
       "  0.9549166560173035,\n",
       "  0.9547500014305115,\n",
       "  0.9559166431427002,\n",
       "  0.9572499990463257,\n",
       "  0.9576666951179504,\n",
       "  0.9580833315849304,\n",
       "  0.9580833315849304,\n",
       "  0.9595833420753479,\n",
       "  0.9601666927337646,\n",
       "  0.9601666927337646,\n",
       "  0.9601666927337646,\n",
       "  0.9609166383743286,\n",
       "  0.9606666564941406,\n",
       "  0.9620833396911621,\n",
       "  0.9624999761581421,\n",
       "  0.9616666436195374,\n",
       "  0.9624999761581421,\n",
       "  0.9634166955947876,\n",
       "  0.9632499814033508,\n",
       "  0.9638333320617676,\n",
       "  0.9642500281333923,\n",
       "  0.9642500281333923,\n",
       "  0.9648333191871643,\n",
       "  0.9644166827201843,\n",
       "  0.9647499918937683,\n",
       "  0.9662500023841858,\n",
       "  0.9650833606719971,\n",
       "  0.965583324432373,\n",
       "  0.965499997138977,\n",
       "  0.965583324432373,\n",
       "  0.9661666750907898,\n",
       "  0.9665833115577698,\n",
       "  0.9664999842643738,\n",
       "  0.9673333168029785,\n",
       "  0.9670833349227905,\n",
       "  0.9672499895095825,\n",
       "  0.9673333168029785,\n",
       "  0.9678333401679993,\n",
       "  0.9677500128746033,\n",
       "  0.9674999713897705,\n",
       "  0.968416690826416,\n",
       "  0.968583345413208,\n",
       "  0.96875,\n",
       "  0.968916654586792,\n",
       "  0.968999981880188,\n",
       "  0.968833327293396,\n",
       "  0.9693333506584167,\n",
       "  0.9693333506584167,\n",
       "  0.9696666598320007,\n",
       "  0.968999981880188,\n",
       "  0.9702500104904175,\n",
       "  0.9704166650772095,\n",
       "  0.9702500104904175,\n",
       "  0.9701666831970215,\n",
       "  0.9706666469573975,\n",
       "  0.9704999923706055,\n",
       "  0.9712499976158142,\n",
       "  0.9703333377838135,\n",
       "  0.9708333611488342,\n",
       "  0.9710833430290222,\n",
       "  0.9704999923706055,\n",
       "  0.9710000157356262,\n",
       "  0.9715833067893982,\n",
       "  0.9714166522026062,\n",
       "  0.971666693687439,\n",
       "  0.9712499976158142,\n",
       "  0.9715833067893982,\n",
       "  0.971666693687439,\n",
       "  0.971833348274231,\n",
       "  0.9713333249092102,\n",
       "  0.971916675567627,\n",
       "  0.9722499847412109,\n",
       "  0.971750020980835,\n",
       "  0.9723333120346069,\n",
       "  0.9727500081062317,\n",
       "  0.9726666808128357,\n",
       "  0.9723333120346069,\n",
       "  0.9727500081062317,\n",
       "  0.9728333353996277,\n",
       "  0.9729166626930237,\n",
       "  0.9726666808128357,\n",
       "  0.9733333587646484,\n",
       "  0.9729166626930237,\n",
       "  0.9733333587646484,\n",
       "  0.9731666445732117,\n",
       "  0.9736666679382324,\n",
       "  0.9731666445732117,\n",
       "  0.9733333587646484,\n",
       "  0.9735833406448364,\n",
       "  0.9738333225250244,\n",
       "  0.9736666679382324,\n",
       "  0.9731666445732117,\n",
       "  0.9737499952316284,\n",
       "  0.9738333225250244,\n",
       "  0.9740833044052124,\n",
       "  0.9736666679382324,\n",
       "  0.9735000133514404,\n",
       "  0.9737499952316284,\n",
       "  0.9735833406448364,\n",
       "  0.9737499952316284,\n",
       "  0.9739166498184204,\n",
       "  0.9745000004768372,\n",
       "  0.9735000133514404,\n",
       "  0.9737499952316284,\n",
       "  0.9735833406448364,\n",
       "  0.9741666913032532,\n",
       "  0.9742500185966492,\n",
       "  0.9744166731834412,\n",
       "  0.9748333096504211,\n",
       "  0.9745000004768372,\n",
       "  0.9746666550636292,\n",
       "  0.9747499823570251,\n",
       "  0.9746666550636292,\n",
       "  0.9749166369438171,\n",
       "  0.9747499823570251,\n",
       "  0.9753333330154419,\n",
       "  0.9745000004768372,\n",
       "  0.9746666550636292,\n",
       "  0.9754166603088379,\n",
       "  0.9743333458900452,\n",
       "  0.9746666550636292,\n",
       "  0.9752500057220459,\n",
       "  0.9746666550636292,\n",
       "  0.9754166603088379,\n",
       "  0.9751666784286499,\n",
       "  0.9752500057220459,\n",
       "  0.9746666550636292,\n",
       "  0.9753333330154419,\n",
       "  0.9749166369438171,\n",
       "  0.9752500057220459,\n",
       "  0.9750833511352539,\n",
       "  0.9757500290870667,\n",
       "  0.9750000238418579,\n",
       "  0.9753333330154419,\n",
       "  0.9754999876022339,\n",
       "  0.9756666421890259,\n",
       "  0.9752500057220459,\n",
       "  0.9752500057220459,\n",
       "  0.9750000238418579,\n",
       "  0.9753333330154419,\n",
       "  0.9754999876022339,\n",
       "  0.9754166603088379,\n",
       "  0.9759166836738586,\n",
       "  0.9754999876022339,\n",
       "  0.9757500290870667,\n",
       "  0.9755833148956299,\n",
       "  0.9754166603088379,\n",
       "  0.9753333330154419,\n",
       "  0.9756666421890259,\n",
       "  0.9753333330154419,\n",
       "  0.9757500290870667,\n",
       "  0.9753333330154419,\n",
       "  0.9755833148956299,\n",
       "  0.9759166836738586,\n",
       "  0.9757500290870667,\n",
       "  0.9757500290870667,\n",
       "  0.9756666421890259,\n",
       "  0.9756666421890259,\n",
       "  0.9759166836738586,\n",
       "  0.9760000109672546,\n",
       "  0.9761666655540466,\n",
       "  0.9761666655540466,\n",
       "  0.9759166836738586,\n",
       "  0.9758333563804626,\n",
       "  0.9759166836738586,\n",
       "  0.9760833382606506,\n",
       "  0.9760833382606506,\n",
       "  0.9760000109672546,\n",
       "  0.9764999747276306,\n",
       "  0.9760000109672546,\n",
       "  0.9764166474342346,\n",
       "  0.9760000109672546]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_v3.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron - Versão 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando outros otimizadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos pacotes e funções\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garantindo que o resultado pode ser reproduzido\n",
    "np.random.seed(1671)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros da rede e do treinamento\n",
    "# NB_EPOCH = 20\n",
    "# BATCH_SIZE = 128\n",
    "# VERBOSE = 1\n",
    "# NB_CLASSES = 10   # número de outputs = número de dígitos\n",
    "# #OPTIMIZER = RMSprop() # otimizador\n",
    "# OPTIMIZER = Adam() # otimizador\n",
    "# N_HIDDEN = 128\n",
    "# VALIDATION_SPLIT=0.2 # quanto é reservado para validação\n",
    "# DROPOUT = 0.3\n",
    "\n",
    "\n",
    "# Parâmetros da rede e do treinamento\n",
    "\n",
    "NB_EPOCH = 200 #Número de épocas\n",
    "\n",
    "BATCH_SIZE = 128 #Tamanho do Batch\n",
    "\n",
    "VERBOSE = 1 #Verbose\n",
    "\n",
    "NB_CLASSES = 10   #Número de outputs = número de dígitos\n",
    "\n",
    "OPTIMIZER = tensorflow.keras.optimizers.Adam() #Otimizador Adam\n",
    "\n",
    "#OPTIMIZER = tensorflow.keras.optimizers.RMSprop() #Otimizador RMSprop\n",
    "\n",
    "N_HIDDEN = 128  #Número de neurônios ocultos\n",
    "\n",
    "VALIDATION_SPLIT = 0.2 #fração de validação\n",
    "\n",
    "DROPOUT = 0.3 #DropOut dos neurônios entre as camadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando datasets de treino e teste\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train possui 60000 linhas de valores 28x28 --> reshape para 60000 x 784\n",
    "RESHAPED = 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 exemplos de treinamento\n",
      "10000 exemplos de teste\n"
     ]
    }
   ],
   "source": [
    "# Normalizando os dados\n",
    "# Tipicamente, os valores associados a cada pixel são normalizados na faixa [0, 1] \n",
    "# (o que significa que a intensidade de cada pixel é dividida por 255, o valor de intensidade máxima). \n",
    "# A saída é 10 classes, uma para cada dígito.\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'exemplos de treinamento')\n",
    "print(X_test.shape[0], 'exemplos de teste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte os vetores da class para matrizes binárias das classes\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria as camadas \n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape = (RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sumário da rede\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compila o modelo\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = OPTIMIZER, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5216 - accuracy: 0.8412 - val_loss: 0.1808 - val_accuracy: 0.9484\n",
      "Epoch 2/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2287 - accuracy: 0.9322 - val_loss: 0.1349 - val_accuracy: 0.9598\n",
      "Epoch 3/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1773 - accuracy: 0.9465 - val_loss: 0.1111 - val_accuracy: 0.9663\n",
      "Epoch 4/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1474 - accuracy: 0.9557 - val_loss: 0.1043 - val_accuracy: 0.9689\n",
      "Epoch 5/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1285 - accuracy: 0.9609 - val_loss: 0.0979 - val_accuracy: 0.9697\n",
      "Epoch 6/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1193 - accuracy: 0.9633 - val_loss: 0.0934 - val_accuracy: 0.9732\n",
      "Epoch 7/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1050 - accuracy: 0.9679 - val_loss: 0.0902 - val_accuracy: 0.9740\n",
      "Epoch 8/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0968 - accuracy: 0.9700 - val_loss: 0.0849 - val_accuracy: 0.9740\n",
      "Epoch 9/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0882 - accuracy: 0.9719 - val_loss: 0.0853 - val_accuracy: 0.9747\n",
      "Epoch 10/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0831 - accuracy: 0.9732 - val_loss: 0.0875 - val_accuracy: 0.9743\n",
      "Epoch 11/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0792 - accuracy: 0.9752 - val_loss: 0.0825 - val_accuracy: 0.9756\n",
      "Epoch 12/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0701 - accuracy: 0.9772 - val_loss: 0.0799 - val_accuracy: 0.9766\n",
      "Epoch 13/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0718 - accuracy: 0.9771 - val_loss: 0.0776 - val_accuracy: 0.9787\n",
      "Epoch 14/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0677 - accuracy: 0.9780 - val_loss: 0.0826 - val_accuracy: 0.9764\n",
      "Epoch 15/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0628 - accuracy: 0.9807 - val_loss: 0.0782 - val_accuracy: 0.9771\n",
      "Epoch 16/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0631 - accuracy: 0.9799 - val_loss: 0.0779 - val_accuracy: 0.9773\n",
      "Epoch 17/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0590 - accuracy: 0.9811 - val_loss: 0.0820 - val_accuracy: 0.9780\n",
      "Epoch 18/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0589 - accuracy: 0.9803 - val_loss: 0.0824 - val_accuracy: 0.9779\n",
      "Epoch 19/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0529 - accuracy: 0.9827 - val_loss: 0.0869 - val_accuracy: 0.9777\n",
      "Epoch 20/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0531 - accuracy: 0.9829 - val_loss: 0.0817 - val_accuracy: 0.9787\n",
      "Epoch 21/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0517 - accuracy: 0.9830 - val_loss: 0.0849 - val_accuracy: 0.9779\n",
      "Epoch 22/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0511 - accuracy: 0.9830 - val_loss: 0.0792 - val_accuracy: 0.9795\n",
      "Epoch 23/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0476 - accuracy: 0.9843 - val_loss: 0.0844 - val_accuracy: 0.9797\n",
      "Epoch 24/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0472 - accuracy: 0.9847 - val_loss: 0.0832 - val_accuracy: 0.9789\n",
      "Epoch 25/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0465 - accuracy: 0.9845 - val_loss: 0.0796 - val_accuracy: 0.9792\n",
      "Epoch 26/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0450 - accuracy: 0.9851 - val_loss: 0.0849 - val_accuracy: 0.9772\n",
      "Epoch 27/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0448 - accuracy: 0.9849 - val_loss: 0.0820 - val_accuracy: 0.9794\n",
      "Epoch 28/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0420 - accuracy: 0.9858 - val_loss: 0.0898 - val_accuracy: 0.9790\n",
      "Epoch 29/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0412 - accuracy: 0.9864 - val_loss: 0.0865 - val_accuracy: 0.9784\n",
      "Epoch 30/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0421 - accuracy: 0.9857 - val_loss: 0.0819 - val_accuracy: 0.9796\n",
      "Epoch 31/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0401 - accuracy: 0.9865 - val_loss: 0.0848 - val_accuracy: 0.9799\n",
      "Epoch 32/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0416 - accuracy: 0.9861 - val_loss: 0.0845 - val_accuracy: 0.9793\n",
      "Epoch 33/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0393 - accuracy: 0.9869 - val_loss: 0.0851 - val_accuracy: 0.9787\n",
      "Epoch 34/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0381 - accuracy: 0.9875 - val_loss: 0.0873 - val_accuracy: 0.9784\n",
      "Epoch 35/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0358 - accuracy: 0.9881 - val_loss: 0.0868 - val_accuracy: 0.9787\n",
      "Epoch 36/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0379 - accuracy: 0.9871 - val_loss: 0.0860 - val_accuracy: 0.9803\n",
      "Epoch 37/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0365 - accuracy: 0.9879 - val_loss: 0.0908 - val_accuracy: 0.9783\n",
      "Epoch 38/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0353 - accuracy: 0.9883 - val_loss: 0.0871 - val_accuracy: 0.9797\n",
      "Epoch 39/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0385 - accuracy: 0.9874 - val_loss: 0.0819 - val_accuracy: 0.9807\n",
      "Epoch 40/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0368 - accuracy: 0.9881 - val_loss: 0.0864 - val_accuracy: 0.9804\n",
      "Epoch 41/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0341 - accuracy: 0.9886 - val_loss: 0.0882 - val_accuracy: 0.9790\n",
      "Epoch 42/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0306 - accuracy: 0.9896 - val_loss: 0.0895 - val_accuracy: 0.9796\n",
      "Epoch 43/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0339 - accuracy: 0.9885 - val_loss: 0.0923 - val_accuracy: 0.9793\n",
      "Epoch 44/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0329 - accuracy: 0.9891 - val_loss: 0.0899 - val_accuracy: 0.9812\n",
      "Epoch 45/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0323 - accuracy: 0.9895 - val_loss: 0.0845 - val_accuracy: 0.9818\n",
      "Epoch 46/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0328 - accuracy: 0.9888 - val_loss: 0.0881 - val_accuracy: 0.9800\n",
      "Epoch 47/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0329 - accuracy: 0.9889 - val_loss: 0.0861 - val_accuracy: 0.9812\n",
      "Epoch 48/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0317 - accuracy: 0.9894 - val_loss: 0.0887 - val_accuracy: 0.9789\n",
      "Epoch 49/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0299 - accuracy: 0.9900 - val_loss: 0.0875 - val_accuracy: 0.9805\n",
      "Epoch 50/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0303 - accuracy: 0.9904 - val_loss: 0.0908 - val_accuracy: 0.9805\n",
      "Epoch 51/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0331 - accuracy: 0.9890 - val_loss: 0.0942 - val_accuracy: 0.9795\n",
      "Epoch 52/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0320 - accuracy: 0.9900 - val_loss: 0.0893 - val_accuracy: 0.9807\n",
      "Epoch 53/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0308 - accuracy: 0.9897 - val_loss: 0.0863 - val_accuracy: 0.9813\n",
      "Epoch 54/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0303 - accuracy: 0.9898 - val_loss: 0.0952 - val_accuracy: 0.9793\n",
      "Epoch 55/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0282 - accuracy: 0.9907 - val_loss: 0.0930 - val_accuracy: 0.9798\n",
      "Epoch 56/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0291 - accuracy: 0.9902 - val_loss: 0.0876 - val_accuracy: 0.9808\n",
      "Epoch 57/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0291 - accuracy: 0.9904 - val_loss: 0.0913 - val_accuracy: 0.9797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0269 - accuracy: 0.9907 - val_loss: 0.0870 - val_accuracy: 0.9806\n",
      "Epoch 59/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0294 - accuracy: 0.9898 - val_loss: 0.0914 - val_accuracy: 0.9812\n",
      "Epoch 60/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0301 - accuracy: 0.9898 - val_loss: 0.0940 - val_accuracy: 0.9808\n",
      "Epoch 61/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0274 - accuracy: 0.9905 - val_loss: 0.0883 - val_accuracy: 0.9814\n",
      "Epoch 62/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0263 - accuracy: 0.9914 - val_loss: 0.0897 - val_accuracy: 0.9803\n",
      "Epoch 63/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0285 - accuracy: 0.9910 - val_loss: 0.0959 - val_accuracy: 0.9797\n",
      "Epoch 64/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0254 - accuracy: 0.9918 - val_loss: 0.0948 - val_accuracy: 0.9797\n",
      "Epoch 65/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0284 - accuracy: 0.9905 - val_loss: 0.0914 - val_accuracy: 0.9797\n",
      "Epoch 66/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0290 - accuracy: 0.9902 - val_loss: 0.0996 - val_accuracy: 0.9795\n",
      "Epoch 67/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0265 - accuracy: 0.9912 - val_loss: 0.0992 - val_accuracy: 0.9800\n",
      "Epoch 68/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0259 - accuracy: 0.9914 - val_loss: 0.0949 - val_accuracy: 0.9803\n",
      "Epoch 69/200\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.99 - 1s 3ms/step - loss: 0.0251 - accuracy: 0.9918 - val_loss: 0.0948 - val_accuracy: 0.9809\n",
      "Epoch 70/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0260 - accuracy: 0.9910 - val_loss: 0.0985 - val_accuracy: 0.9803\n",
      "Epoch 71/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0272 - accuracy: 0.9912 - val_loss: 0.0974 - val_accuracy: 0.9798\n",
      "Epoch 72/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0264 - accuracy: 0.9909 - val_loss: 0.0936 - val_accuracy: 0.9795\n",
      "Epoch 73/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0249 - accuracy: 0.9913 - val_loss: 0.0966 - val_accuracy: 0.9797\n",
      "Epoch 74/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0266 - accuracy: 0.9916 - val_loss: 0.0974 - val_accuracy: 0.9796\n",
      "Epoch 75/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0247 - accuracy: 0.9917 - val_loss: 0.0952 - val_accuracy: 0.9799\n",
      "Epoch 76/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0237 - accuracy: 0.9920 - val_loss: 0.0912 - val_accuracy: 0.9811\n",
      "Epoch 77/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0269 - accuracy: 0.9914 - val_loss: 0.0952 - val_accuracy: 0.9803\n",
      "Epoch 78/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0237 - accuracy: 0.9920 - val_loss: 0.0907 - val_accuracy: 0.9803\n",
      "Epoch 79/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0234 - accuracy: 0.9921 - val_loss: 0.0991 - val_accuracy: 0.9803\n",
      "Epoch 80/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0284 - accuracy: 0.9909 - val_loss: 0.1003 - val_accuracy: 0.9797\n",
      "Epoch 81/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0253 - accuracy: 0.9920 - val_loss: 0.0949 - val_accuracy: 0.9804\n",
      "Epoch 82/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0263 - accuracy: 0.9910 - val_loss: 0.0960 - val_accuracy: 0.9800\n",
      "Epoch 83/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0228 - accuracy: 0.9925 - val_loss: 0.0989 - val_accuracy: 0.9801\n",
      "Epoch 84/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0229 - accuracy: 0.9926 - val_loss: 0.0983 - val_accuracy: 0.9804\n",
      "Epoch 85/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0239 - accuracy: 0.9926 - val_loss: 0.1014 - val_accuracy: 0.9799\n",
      "Epoch 86/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0235 - accuracy: 0.9923 - val_loss: 0.0987 - val_accuracy: 0.9790\n",
      "Epoch 87/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0260 - accuracy: 0.9913 - val_loss: 0.0972 - val_accuracy: 0.9796\n",
      "Epoch 88/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0216 - accuracy: 0.9927 - val_loss: 0.0998 - val_accuracy: 0.9791\n",
      "Epoch 89/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0241 - accuracy: 0.9920 - val_loss: 0.1030 - val_accuracy: 0.9807\n",
      "Epoch 90/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0234 - accuracy: 0.9924 - val_loss: 0.1064 - val_accuracy: 0.9797\n",
      "Epoch 91/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0223 - accuracy: 0.9927 - val_loss: 0.1040 - val_accuracy: 0.9798\n",
      "Epoch 92/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0223 - accuracy: 0.9925 - val_loss: 0.1056 - val_accuracy: 0.9805\n",
      "Epoch 93/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0222 - accuracy: 0.9930 - val_loss: 0.1045 - val_accuracy: 0.9797\n",
      "Epoch 94/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0213 - accuracy: 0.9929 - val_loss: 0.1027 - val_accuracy: 0.9796\n",
      "Epoch 95/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0238 - accuracy: 0.9926 - val_loss: 0.1072 - val_accuracy: 0.9803\n",
      "Epoch 96/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0234 - accuracy: 0.9926 - val_loss: 0.1037 - val_accuracy: 0.9780\n",
      "Epoch 97/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0226 - accuracy: 0.9928 - val_loss: 0.1023 - val_accuracy: 0.9809\n",
      "Epoch 98/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0215 - accuracy: 0.9930 - val_loss: 0.1051 - val_accuracy: 0.9805\n",
      "Epoch 99/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0218 - accuracy: 0.9928 - val_loss: 0.1083 - val_accuracy: 0.9797\n",
      "Epoch 100/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0245 - accuracy: 0.9922 - val_loss: 0.1050 - val_accuracy: 0.9803\n",
      "Epoch 101/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0229 - accuracy: 0.9925 - val_loss: 0.1122 - val_accuracy: 0.9801\n",
      "Epoch 102/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0221 - accuracy: 0.9924 - val_loss: 0.1024 - val_accuracy: 0.9809\n",
      "Epoch 103/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0217 - accuracy: 0.9929 - val_loss: 0.1102 - val_accuracy: 0.9794\n",
      "Epoch 104/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0241 - accuracy: 0.9925 - val_loss: 0.1004 - val_accuracy: 0.9812\n",
      "Epoch 105/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0231 - accuracy: 0.9925 - val_loss: 0.1044 - val_accuracy: 0.9808\n",
      "Epoch 106/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0222 - accuracy: 0.9931 - val_loss: 0.1031 - val_accuracy: 0.9806\n",
      "Epoch 107/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0212 - accuracy: 0.9928 - val_loss: 0.1035 - val_accuracy: 0.9811\n",
      "Epoch 108/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0225 - accuracy: 0.9926 - val_loss: 0.1008 - val_accuracy: 0.9808\n",
      "Epoch 109/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0200 - accuracy: 0.9937 - val_loss: 0.1045 - val_accuracy: 0.9798\n",
      "Epoch 110/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0230 - accuracy: 0.9927 - val_loss: 0.0985 - val_accuracy: 0.9806\n",
      "Epoch 111/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0199 - accuracy: 0.9935 - val_loss: 0.1085 - val_accuracy: 0.9802\n",
      "Epoch 112/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0205 - accuracy: 0.9934 - val_loss: 0.1049 - val_accuracy: 0.9798\n",
      "Epoch 113/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0198 - accuracy: 0.9937 - val_loss: 0.1067 - val_accuracy: 0.9812\n",
      "Epoch 114/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0236 - accuracy: 0.9926 - val_loss: 0.1001 - val_accuracy: 0.9818\n",
      "Epoch 115/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0210 - accuracy: 0.9928 - val_loss: 0.1068 - val_accuracy: 0.9821\n",
      "Epoch 116/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0203 - accuracy: 0.9933 - val_loss: 0.0996 - val_accuracy: 0.9811\n",
      "Epoch 117/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0192 - accuracy: 0.9940 - val_loss: 0.1087 - val_accuracy: 0.9809\n",
      "Epoch 118/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0205 - accuracy: 0.9931 - val_loss: 0.1070 - val_accuracy: 0.9806\n",
      "Epoch 119/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0211 - accuracy: 0.9936 - val_loss: 0.1116 - val_accuracy: 0.9788\n",
      "Epoch 120/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0208 - accuracy: 0.9939 - val_loss: 0.1029 - val_accuracy: 0.9807\n",
      "Epoch 121/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0165 - accuracy: 0.9948 - val_loss: 0.1094 - val_accuracy: 0.9798\n",
      "Epoch 122/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.1121 - val_accuracy: 0.9810\n",
      "Epoch 123/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0207 - accuracy: 0.9931 - val_loss: 0.1126 - val_accuracy: 0.9804\n",
      "Epoch 124/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0194 - accuracy: 0.9936 - val_loss: 0.1159 - val_accuracy: 0.9803\n",
      "Epoch 125/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0194 - accuracy: 0.9939 - val_loss: 0.1126 - val_accuracy: 0.9791\n",
      "Epoch 126/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0220 - accuracy: 0.9931 - val_loss: 0.1171 - val_accuracy: 0.9797\n",
      "Epoch 127/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0217 - accuracy: 0.9931 - val_loss: 0.1111 - val_accuracy: 0.9796\n",
      "Epoch 128/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0204 - accuracy: 0.9935 - val_loss: 0.1106 - val_accuracy: 0.9803\n",
      "Epoch 129/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0212 - accuracy: 0.9932 - val_loss: 0.1104 - val_accuracy: 0.9801\n",
      "Epoch 130/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0210 - accuracy: 0.9929 - val_loss: 0.1094 - val_accuracy: 0.9806\n",
      "Epoch 131/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0211 - accuracy: 0.9934 - val_loss: 0.1082 - val_accuracy: 0.9798\n",
      "Epoch 132/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0217 - accuracy: 0.9931 - val_loss: 0.1068 - val_accuracy: 0.9817\n",
      "Epoch 133/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0203 - accuracy: 0.9935 - val_loss: 0.1168 - val_accuracy: 0.9811\n",
      "Epoch 134/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0185 - accuracy: 0.9934 - val_loss: 0.1123 - val_accuracy: 0.9805\n",
      "Epoch 135/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0195 - accuracy: 0.9941 - val_loss: 0.1075 - val_accuracy: 0.9804\n",
      "Epoch 136/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0175 - accuracy: 0.9944 - val_loss: 0.1145 - val_accuracy: 0.9804\n",
      "Epoch 137/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0197 - accuracy: 0.9935 - val_loss: 0.1257 - val_accuracy: 0.9795\n",
      "Epoch 138/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0202 - accuracy: 0.9936 - val_loss: 0.1163 - val_accuracy: 0.9797\n",
      "Epoch 139/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0191 - accuracy: 0.9938 - val_loss: 0.1105 - val_accuracy: 0.9807\n",
      "Epoch 140/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0194 - accuracy: 0.9937 - val_loss: 0.1142 - val_accuracy: 0.9810\n",
      "Epoch 141/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0209 - accuracy: 0.9934 - val_loss: 0.1160 - val_accuracy: 0.9787\n",
      "Epoch 142/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0171 - accuracy: 0.9941 - val_loss: 0.1183 - val_accuracy: 0.9795\n",
      "Epoch 143/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0187 - accuracy: 0.9939 - val_loss: 0.1137 - val_accuracy: 0.9809\n",
      "Epoch 144/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0174 - accuracy: 0.9945 - val_loss: 0.1157 - val_accuracy: 0.9802\n",
      "Epoch 145/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 0.1247 - val_accuracy: 0.9792\n",
      "Epoch 146/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0168 - accuracy: 0.9944 - val_loss: 0.1151 - val_accuracy: 0.9800\n",
      "Epoch 147/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0209 - accuracy: 0.9935 - val_loss: 0.1108 - val_accuracy: 0.9809\n",
      "Epoch 148/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0204 - accuracy: 0.9931 - val_loss: 0.1103 - val_accuracy: 0.9787\n",
      "Epoch 149/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0180 - accuracy: 0.9945 - val_loss: 0.1139 - val_accuracy: 0.9803\n",
      "Epoch 150/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0169 - accuracy: 0.9948 - val_loss: 0.1210 - val_accuracy: 0.9805\n",
      "Epoch 151/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0184 - accuracy: 0.9939 - val_loss: 0.1102 - val_accuracy: 0.9810\n",
      "Epoch 152/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0167 - accuracy: 0.9943 - val_loss: 0.1199 - val_accuracy: 0.9797\n",
      "Epoch 153/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0194 - accuracy: 0.9941 - val_loss: 0.1220 - val_accuracy: 0.9798\n",
      "Epoch 154/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0201 - accuracy: 0.9937 - val_loss: 0.1198 - val_accuracy: 0.9809\n",
      "Epoch 155/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0199 - accuracy: 0.9932 - val_loss: 0.1228 - val_accuracy: 0.9797\n",
      "Epoch 156/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0197 - accuracy: 0.9940 - val_loss: 0.1168 - val_accuracy: 0.9801\n",
      "Epoch 157/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.1225 - val_accuracy: 0.9796\n",
      "Epoch 158/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0177 - accuracy: 0.9945 - val_loss: 0.1251 - val_accuracy: 0.9806\n",
      "Epoch 159/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0166 - accuracy: 0.9946 - val_loss: 0.1171 - val_accuracy: 0.9804\n",
      "Epoch 160/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0188 - accuracy: 0.9942 - val_loss: 0.1193 - val_accuracy: 0.9806\n",
      "Epoch 161/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0191 - accuracy: 0.9937 - val_loss: 0.1242 - val_accuracy: 0.9797\n",
      "Epoch 162/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0187 - accuracy: 0.9945 - val_loss: 0.1258 - val_accuracy: 0.9795\n",
      "Epoch 163/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0183 - accuracy: 0.9940 - val_loss: 0.1143 - val_accuracy: 0.9810\n",
      "Epoch 164/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0174 - accuracy: 0.9941 - val_loss: 0.1247 - val_accuracy: 0.9799\n",
      "Epoch 165/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0185 - accuracy: 0.9941 - val_loss: 0.1180 - val_accuracy: 0.9789\n",
      "Epoch 166/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0174 - accuracy: 0.9945 - val_loss: 0.1172 - val_accuracy: 0.9798\n",
      "Epoch 167/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0174 - accuracy: 0.9943 - val_loss: 0.1190 - val_accuracy: 0.9800\n",
      "Epoch 168/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0171 - accuracy: 0.9945 - val_loss: 0.1233 - val_accuracy: 0.9803\n",
      "Epoch 169/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0187 - accuracy: 0.9941 - val_loss: 0.1199 - val_accuracy: 0.9795\n",
      "Epoch 170/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0175 - accuracy: 0.9943 - val_loss: 0.1282 - val_accuracy: 0.9788\n",
      "Epoch 171/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0177 - accuracy: 0.9947 - val_loss: 0.1333 - val_accuracy: 0.9791\n",
      "Epoch 172/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0192 - accuracy: 0.9941 - val_loss: 0.1217 - val_accuracy: 0.9791\n",
      "Epoch 173/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0179 - accuracy: 0.9939 - val_loss: 0.1295 - val_accuracy: 0.9790\n",
      "Epoch 174/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0180 - accuracy: 0.9943 - val_loss: 0.1244 - val_accuracy: 0.9792\n",
      "Epoch 175/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0172 - accuracy: 0.9944 - val_loss: 0.1236 - val_accuracy: 0.9795\n",
      "Epoch 176/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0174 - accuracy: 0.9944 - val_loss: 0.1272 - val_accuracy: 0.9802\n",
      "Epoch 177/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0174 - accuracy: 0.9944 - val_loss: 0.1196 - val_accuracy: 0.9799\n",
      "Epoch 178/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0161 - accuracy: 0.9948 - val_loss: 0.1201 - val_accuracy: 0.9800\n",
      "Epoch 179/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0198 - accuracy: 0.9937 - val_loss: 0.1187 - val_accuracy: 0.9797\n",
      "Epoch 180/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0188 - accuracy: 0.9942 - val_loss: 0.1173 - val_accuracy: 0.9802\n",
      "Epoch 181/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0180 - accuracy: 0.9945 - val_loss: 0.1208 - val_accuracy: 0.9798\n",
      "Epoch 182/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0170 - accuracy: 0.9948 - val_loss: 0.1204 - val_accuracy: 0.9806\n",
      "Epoch 183/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0173 - accuracy: 0.9950 - val_loss: 0.1256 - val_accuracy: 0.9791\n",
      "Epoch 184/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0176 - accuracy: 0.9946 - val_loss: 0.1190 - val_accuracy: 0.9802\n",
      "Epoch 185/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0171 - accuracy: 0.9945 - val_loss: 0.1139 - val_accuracy: 0.9803\n",
      "Epoch 186/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0185 - accuracy: 0.9948 - val_loss: 0.1194 - val_accuracy: 0.9803\n",
      "Epoch 187/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0195 - accuracy: 0.9936 - val_loss: 0.1177 - val_accuracy: 0.9792\n",
      "Epoch 188/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 0.9941 - val_loss: 0.1137 - val_accuracy: 0.9809\n",
      "Epoch 189/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0196 - accuracy: 0.9938 - val_loss: 0.1182 - val_accuracy: 0.9810\n",
      "Epoch 190/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0167 - accuracy: 0.9950 - val_loss: 0.1205 - val_accuracy: 0.9807\n",
      "Epoch 191/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0165 - accuracy: 0.9947 - val_loss: 0.1134 - val_accuracy: 0.9806\n",
      "Epoch 192/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0170 - accuracy: 0.9948 - val_loss: 0.1204 - val_accuracy: 0.9807\n",
      "Epoch 193/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.1220 - val_accuracy: 0.9795\n",
      "Epoch 194/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0175 - accuracy: 0.9944 - val_loss: 0.1164 - val_accuracy: 0.9805\n",
      "Epoch 195/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0157 - accuracy: 0.9948 - val_loss: 0.1231 - val_accuracy: 0.9803\n",
      "Epoch 196/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0177 - accuracy: 0.9942 - val_loss: 0.1206 - val_accuracy: 0.9807\n",
      "Epoch 197/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0153 - accuracy: 0.9952 - val_loss: 0.1171 - val_accuracy: 0.9799\n",
      "Epoch 198/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0160 - accuracy: 0.9948 - val_loss: 0.1220 - val_accuracy: 0.9809\n",
      "Epoch 199/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0148 - accuracy: 0.9955 - val_loss: 0.1190 - val_accuracy: 0.9803\n",
      "Epoch 200/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0169 - accuracy: 0.9949 - val_loss: 0.1240 - val_accuracy: 0.9793\n"
     ]
    }
   ],
   "source": [
    "# Treinamento do modelo\n",
    "modelo_v4 = model.fit(X_train, Y_train,\n",
    "                      batch_size = BATCH_SIZE, \n",
    "                      epochs = NB_EPOCH,\n",
    "                      verbose = VERBOSE, \n",
    "                      validation_split = VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 2ms/step - loss: 0.1226 - accuracy: 0.9800\n",
      "\n",
      "Test score: 0.12257346510887146\n",
      "Test accuracy: 0.9800000190734863\n"
     ]
    }
   ],
   "source": [
    "# Testa o modelo e imprime o score\n",
    "score = model.evaluate(X_test, Y_test, verbose = VERBOSE)\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imprime os dados no modelo\n",
    "print(modelo_v4.history.keys())\n",
    "\n",
    "# Sumariza o modelo para acurácia\n",
    "plt.plot(modelo_v4.history['accuracy'])\n",
    "plt.plot(modelo_v4.history['val_accuracy'])\n",
    "plt.title('Acurácia do Modelo')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Treino', 'Teste'], loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA4s0lEQVR4nO3dd5xU1fn48c8zM9srW6hLWbqgCwhiQUE0KqiJvZdoLEGjUfNLokm+STSmWKJGownRSIzG3gJGLKCIICJF6XWpuyxlC9v7zPn9ce7uzjZcyuwA93m/Xvti5paZZ+4M57mn3HPFGINSSin38oQ7AKWUUuGliUAppVxOE4FSSrmcJgKllHI5TQRKKeVymgiUUsrlNBEo9S1EpJ+IGBHxdWDbG0RkfmfEpdShoolAHVVEZKuI1IpIWovly5zCvF+YQtuvhKJUZ9JEoI5GW4CrGp6IyHFATPjCUerwpolAHY1eAq4Pev594MXgDUQkSUReFJF8EdkmIv8nIh5nnVdE/iwiBSKyGTivjX2fF5GdIrJDRH4vIt6DCVhEeorIDBEpEpFsEbklaN1YEVkiIqUisltEHneWR4vIf0SkUESKRWSxiHQ7mDiUO2kiUEejhUCiiBzjFNBXAP9psc1fgSSgPzABmzhudNbdApwPjALGAJe22PffQD0w0NnmbODmg4z5VSAX6Om83x9F5Exn3ZPAk8aYRGAA8Iaz/PvOZ+gNpAJTgKqDjEO5kCYCdbRqqBWcBawDdjSsCEoOvzDGlBljtgKPAdc5m1wO/MUYk2OMKQL+FLRvN2AycLcxpsIYswd4ArjyQAMVkd7AqcC9xphqY8wy4J9B8dQBA0UkzRhTboxZGLQ8FRhojPEbY5YaY0oPNA7lXpoI1NHqJeBq4AZaNAsBaUAksC1o2Tagl/O4J5DTYl2DvkAEsNNpjikG/gF0PYhYewJFxpiyduK5CRgMrHOaf853lr8EfAS8JiJ5IvKIiEQcRBzKpTQRqKOSMWYbttP4XOCdFqsLsGfTfYOW9aGp1rAT29wSvK5BDlADpBljkp2/RGPM8IMINw9IEZGEtuIxxmw0xlyFTTYPA2+JSJwxps4Y84AxZhhwCrY563qU2k+aCNTR7CbgDGNMRfBCY4wf287+BxFJEJG+wE9o6kd4A/ixiGSISBfgvqB9dwIfA4+JSKKIeERkgIhM2I+4opyO3mgRicYW+AuAPznLspzYXwYQkWtFJN0YEwCKndfwi8hEETnOaeoqxSY3/37EoRSgiUAdxYwxm4wxS9pZfSdQAWwG5gOvANOcdc9hm1yWA1/TukZxPbZpaQ2wF3gL6LEfoZVjO3Ub/s7ADnfth60dvAv81hgzy9l+ErBaRMqxHcdXGmOqge7Oe5cCa4G5tO4UV+pbid6YRiml3E1rBEop5XKaCJRSyuU0ESillMtpIlBKKZc74mZBTEtLM/369Qt3GEopdURZunRpgTEmva11R1wi6NevH0uWtDciUCmlVFtEZFt767RpSCmlXE4TgVJKuZwmAqWUcrkjro9AKaXaU1dXR25uLtXV1eEOJWyio6PJyMggIqLjE9FqIlBKHTVyc3NJSEigX79+iEi4w+l0xhgKCwvJzc0lMzOzw/tp05BS6qhRXV1NamqqK5MAgIiQmpq63zUiTQRKqaOKW5NAgwP5/K5JBOt3lfHYx+spLK8JdyhKKXVYcU0fwab8cv76aTbfHdGT1PiocIejlDoKFRYWcuaZZwKwa9cuvF4v6en2Yt5FixYRGRnZ7r5Tp04lNjaW66/v/JvMuSYReJzqUr1f77+glAqN1NRUli1bBsD9999PfHw8P/3pTxvX19fX4/O1XexOmTKlM0Jsk2uahrwemwgCeiMepVQnuuGGG/jJT37CxIkTuffee9m0aROTJk1i9OjRnHbaaaxbtw6wiePPf/4zAKeffjr33nsvY8eOZfDgwcybNw+wneE33ngjxx13HKNGjWLOnDmHJEbX1Ah8TiKoD2giUMoNHnhvNWvySg/paw7rmchvvzt8v/fbsGEDs2fPxuv1cuaZZzJ16lQGDRrEV199xe23386nn37aap/6+noWLVrEzJkzeeCBB5g9ezbPPPMMACtXrmTdunWcffbZbNiwgejo6IP6XK5JBB4nEfg1ESilOtlll12G1+ulvLycBQsWcNlllzWuq6lpewDLxRdfDMDo0aPZunUrAPPnz+fOO+8EYOjQofTt25cNGzaQlZV1UPG5JhH4tGlIKVc5kDP3UImLiwMgEAiQnJzc2I+wL1FRdlCL1+ulvr4esBeMhUJI+whEZJKIrBeRbBG5r431p4tIiYgsc/5+E6pYtLNYKRVuiYmJZGZm8uabbwK2YF++fHmH9x8/fjwvv/wyYJubtm/fzpAhQw46rpAlAhHxAs8Ak4FhwFUiMqyNTecZY0Y6f78LVTw+r9YIlFLh9/LLL/P8888zYsQIhg8fzvTp0zu87+23347f7+e4447jiiuu4IUXXmisORyMUDYNjQWyjTGbAUTkNeACYE0I37NdjTUC7SNQSnWC+++/v83lmZmZfPjhh/vc/rPPPmt8nJaW1thHEB0dzQsvvHDognSEsmmoF5AT9DzXWdbSySKyXEQ+EJE2G/VE5FYRWSIiS/Lz8w8omMbho5oIlFKqmVAmgrYmvGhZCn8N9DXGjAD+Cvy3rRcyxjxrjBljjBnTcJXe/tLho0op1bZQJoJcoHfQ8wwgL3gDY0ypMabceTwTiBCRtFAE09A0pMNHlVKquVAmgsXAIBHJFJFI4EpgRvAGItJdnKnyRGSsE09hKILRzmKllGpbyDqLjTH1InIH8BHgBaYZY1aLyBRn/VTgUuA2EakHqoArTYgGympnsVJKtS2kF5Q5zT0zWyybGvT4aeDpUMbQwKedxUop1SbXXFns1c5ipVSIHcw01GCHjUZGRnLKKaeEPNZgrkkEHq0RKKVC7Numof42n332GfHx8Z2eCFwzDbUOH1VKhcPSpUuZMGECo0eP5pxzzmHnzp0APPXUUwwbNoysrCyuvPJKtm7dytSpU3niiScYOXIk8+bNIz8/n0suuYQTTjiBE044gS+++CIkMbqnRtAwfFRHDSnlDh/cB7tWHtrX7H4cTH6ow5sbY7jzzjuZPn066enpvP766/zqV79i2rRpPPTQQ2zZsoWoqCiKi4tJTk5mypQpzWoRV199Nffccw+nnnoq27dv55xzzmHt2rWH9jPhokSgncVKqc5WU1PDqlWrOOusswDw+/306NEDgKysLK655houvPBCLrzwwjb3nz17NmvWNM3KU1paSllZGQkJCYc0TtckAo82DSnlLvtx5h4qxhiGDx/Ol19+2Wrd+++/z+eff86MGTN48MEHWb16dattAoEAX375JTExMSGN03V9BFojUEp1lqioKPLz8xsTQV1dHatXryYQCJCTk8PEiRN55JFHKC4upry8nISEBMrKyhr3P/vss3n66aYR9h25j8GBcE0i0OGjSqnO5vF4eOutt7j33nsZMWIEI0eOZMGCBfj9fq699trGew/fc889JCcn893vfpd33323sbP4qaeeYsmSJWRlZTFs2DCmTp367W96ANzTNCQ6xYRSqvMETyv9+eeft1o/f/78VssGDx7MihUrmi17/fXXD3lsLbmmRtA4fFTvUKaUUs24JhE03rxeawRKKdWMaxIB2FqBdhYrdXQL1Q3ejxQH8vldlQg8HtHOYqWOYtHR0RQWFro2GRhjKCwsJDo6er/2c01nMTg1Apf+QJRyg4yMDHJzcznQW9oeDaKjo8nIyNivfVyVCLwi2lms1FEsIiKCzMzMcIdxxHFd05DWCJRSqjlXJQKfR6gPBMIdhlJKHVZclQg8HsGveUAppZpxVSLQ4aNKKdWaqxKBR3T4qFJKteSqRODzamexUkq15KpE4NUagVJKteKqRODRPgKllGrFVYlAh48qpVRrrkoEHtHho0op1ZKrEoF2FiulVGuuSgQ6fFQppVpzVSLQC8qUUqo1VyUCj3YWK6VUKyFNBCIySUTWi0i2iNy3j+1OEBG/iFwayni8ImgeUEqp5kKWCETECzwDTAaGAVeJyLB2tnsY+ChUsTTwebVGoJRSLYWyRjAWyDbGbDbG1AKvARe0sd2dwNvAnhDGAjjDR7WLQCmlmgllIugF5AQ9z3WWNRKRXsBFwNQQxtFIO4uVUqq1UCYCaWNZy1L4L8C9xhj/Pl9I5FYRWSIiSw7mXqR683qllGotlPcszgV6Bz3PAPJabDMGeE1EANKAc0Wk3hjz3+CNjDHPAs8CjBkz5oBLcq0RKKVUa6FMBIuBQSKSCewArgSuDt7AGNN4l2kReQH4X8skcCjp8FGllGotZInAGFMvIndgRwN5gWnGmNUiMsVZ3yn9AsG8ImiFQCmlmgtljQBjzExgZotlbSYAY8wNoYwFdPZRpZRqi+uuLNY8oJRSzbkqEfg8gl/bhpRSqhlXJQIdPqqUUq25KhH4PHo/AqWUaslVicAjQr3eokwppZpxVSLwenT4qFJKteSqRKDDR5VSqjVXJQIdPqqUUq25KhH4PIJfO4uVUqoZVyUCj9jrCIwmA6WUauSqRODz2JmxtcNYKaWauCoReJxEoB3GSinVxFWJwNtQI9A8oJRSjVyVCHxaI1BKqVZclQg8ojUCpZRqyVWJwOe1iUCHkCqlVBNXJYKGGoE2DSmlVBNXJQKfdhYrpVQrrkoEOnxUKaVac1Ui8GpnsVJKteKqRNDQWaw1AqWUauKqRNA4fFRHDSmlVCNXJYKGzmK9SZlSSjVxVSLQzmKllGrNVYlAh48qpVRrrkoEWiNQSqnWXJUIvNpZrJRSrbgqETTOPurXRKCUUg1clQgamoZ00jmllGriqkSgncVKKdVaSBOBiEwSkfUiki0i97Wx/gIRWSEiy0RkiYicGsp4tLNYKaVa84XqhUXECzwDnAXkAotFZIYxZk3QZp8AM4wxRkSygDeAoaGKSTuLlVKqtVDWCMYC2caYzcaYWuA14ILgDYwx5cY0lspxQEhLaK92FiulVCuhTAS9gJyg57nOsmZE5CIRWQe8D/wghPE03bxeawRKKdUolIlA2ljWqgQ2xrxrjBkKXAg82OYLidzq9CEsyc/PP+CAmm5er4lAKaUahDIR5AK9g55nAHntbWyM+RwYICJpbax71hgzxhgzJj09/YADahw+qolAKaUahTIRLAYGiUimiEQCVwIzgjcQkYEitgdXRI4HIoHCUAXk06YhpZRqJWSjhowx9SJyB/AR4AWmGWNWi8gUZ/1U4BLgehGpA6qAK4I6jw+5xpvXa2exUko16lAiEJE4oMoYExCRwdghnh8YY+r2tZ8xZiYws8WyqUGPHwYe3u+oD5B2FiulVGsdbRr6HIgWkV7Ysf83Ai+EKqhQ0c5ipZRqraOJQIwxlcDFwF+NMRcBw0IXVmh4GqeY0ESglFINOpwIRORk4BrseH8IYf9CqGiNQCmlWutoIrgb+AXwrtPh2x+YE7KoQkSHjyqlVGsdOqs3xswF5gKIiAcoMMb8OJSBhYIOH1VKqdY6VCMQkVdEJNEZPbQGWC8iPwttaIde4/BRrREopVSjjjYNDTPGlGKngZgJ9AGuC1VQoeLVzmKllGqlo4kgQkQisIlgunP9wBFXmnq1RqCUUq10NBH8A9iKnSr6cxHpC5SGKqhQ8XgEEa0RKKVUsI52Fj8FPBW0aJuITAxNSKHl84jWCJRSKkhHO4uTROTxhqmgReQxbO3giOMR0ZvXK6VUkI42DU0DyoDLnb9S4F+hCiqUfB7RpiGllArS0auDBxhjLgl6/oCILAtBPCHn0aYhpZRqpqM1gioRObXhiYiMw04bfcTxao1AKaWa6WiNYArwoogkOc/3At8PTUihpZ3FSinVXEdHDS0HRohIovO8VETuBlaEMLaQ8IjoFBNKKRVkv25VaYwpda4wBvhJCOIJOZ9H9A5lSikV5GDuWSyHLIpO5PHo8FGllAp2MIngiCxNdfioUko1t88+AhEpo+0CX4CYkEQUYjp8VCmlmttnIjDGJHRWIJ3Fq53FSinVzME0DR2RvNpZrJRSzbguEUT6PNT5A+EOQymlDhuuSwRJMREUV9WFOwyllDpsuC4RdImNZG9FbbjDUEqpw4YLE0EEeyu1RqCUUg3clwjiIimpqqNe+wmUUgpwYyKIjQSgRPsJlFIKcGMiiLOJYG+l9hMopRS4MRHERgBoP4FSSjlCmghEZJKIrBeRbBG5r43114jICudvgYiMCGU80NQ0VKQjh5RSCghhIhARL/AMMBkYBlwlIsNabLYFmGCMyQIeBJ4NVTwNGpqGirVpSCmlgNDWCMYC2caYzcaYWuA14ILgDYwxC4wxe52nC4GMEMYDQEpjjUCbhpRSCkKbCHoBOUHPc51l7bkJ+CCE8QAQE+klyufRzmKllHJ09J7FB6KtG9e0OdubiEzEJoJT21l/K3ArQJ8+fQ46sJQ4vbpYKaUahLJGkAv0DnqeAeS13EhEsoB/AhcYYwrbeiFjzLPGmDHGmDHp6ekHHVhybKTWCJRSyhHKRLAYGCQimSISCVwJzAjeQET6AO8A1xljNoQwlmZS4nSaCaWUahCypiFjTL2I3AF8BHiBacaY1SIyxVk/FfgNkAr8TUQA6o0xY0IVU4Pk2EjW5pWG+m2UUuqIEMo+AowxM4GZLZZNDXp8M3BzKGNoS4o2DSmlVCPXXVkM9uri4qo6/HrvYqWUclEiyF0C706B8j10iYvEGCjVieeUUspFiaB8Dyx/FUp3NE0zoc1DSinlokQQl2b/rSggxZlmorBcE4FSSrkwEeST0SUGgNy9lWEMSCmlDg8uSgTOhWgV+fTqEoMIbC/SRKCUUu5JBJHx4IuGinyifF66J0ZrIlBKKdyUCERsraCiAIDeXWLJLaoKc1BKKRV+7kkEYPsJKvIB6J0SqzUCpZTCdYkgvTER9EmJZXdZNdV1/jAHpZRS4eXCROA0DaXEYAzsKNbmIaWUu7ksEaTZRGAMfVJiAcjR5iGllMu5LBGkg78GasrorYlAKaUANyYCgIp80uOjiPJ5tMNYKeV6LksETdNMeDxCRpcYcnQIqVLK5VyWCJpqBAD90+PZsLssjAEppVT4uToRjMhIYnNBBSU6HbVSysXclQhim5qGALIykgFYtaMkTAEppVT4uSsR+CIhOqmxRpCVkQTA8tziMAallFLh5a5EAM5FZXsAexP7fqmxLM8pDm9MSqnO568PdwSHDfclgqQM2Lut8WlWRjIrcrVpSClXWfoCPDoACrLDHclhwX2JIH0o5K8HY29cn5WRxM6SavaUVoc5MKVUp8hdAu//FKqLYcXrULUX5vwRKovCHVnYuDARDIG6CijJBWBUny4AzM8uCGdUSh3+KgrDHUHbSnIhEOjYtv56ePtmSOwBvcbA6ndg7qMw92G7POC3J4mLnoMFT3fsNetrGk8sj1QuTARD7b/56wAY1TuZ/mlxvLRw2z52Usrl1v4P/jwI8pZ1bPu6anj3Nti18sDez5imwtUYWDINnh4Led80bbNtATx3BjwxHJY8bwvxd6fAlnmtX698D9RV2YJ/7xaY9BCMuhYKs+GrqZDSHzZ9Am/fBNPvgJk/hY//D3av3nece7fC48PstgfLX29jbM/cR2DnioN/nza4PhF4PML1J/flm+3FrNDRQ0q1bdnLYPz2TLkjVr0Fy1+xhVdb6qpgx9Km5yU74MNfwrzHYOdymHoqvHG9TQLv/hD+dw8UbIBP/2C3L9gIr1xhRwAmZtgmnk1zYPmrMOMOm4ga5H0DT42CZ0+Hzx+1ZcDgyXDM90C8dptr34aT74ANH8Gy/8CYH0BUYtP7tVS0GXatgjdvgMoCWPi3jiW9mqALWAuybQKZ97h9/u4P4c+DYfHztoZTV22bsNa9D8tehTl/gNXvfvt7HAAxR1iVZsyYMWbJkiUH9yKPDoLBZ8MFzwBQVl3HSX/8hHOO7c7jl488+CCVOppU7bX/ZzxOofmTtRCb0nybigLw19kmF2NsQb57lS1o71ltlwd77y5Y+m+4cymU74aXLoJAvf0D8EbZCSKHXQhr/gun/RQiYuDTB+GiZ22BXrUXbv0MVr4JnzwAvU+0SaS+Gs74P7vP1nnw5o32NrW1ZVBdAhf9A0Zcad/ng/sgKt5uD/asvGovxKfbJqM5v7fXH/mi4diL7X7bFsDMnwFO2XnBMzDrN5A2GG6YaU8yF/wVjrsUBpxh7464dxt89EtYPxOueQsiYuGF82xyBTjpR7DwGUjqDSU59nNHxtukBOCJgD4nwXX/Ba/vgL5GEVlqjBnT1roDe8UjXfoQ22HsSIiO4PysnsxcuZN6fwCf130VJXWUCfhtgdX7RHvG+tEv4bT/B92P2/d+hZvs2f/4n0NEtF229j0I1MH5T9iz7W/+A+N+bNfVVthawuePgjcCbpoNpTtsEjj1JzD/cfjmJZjw86b32LMWvn4RMLYQ37HUXt9z0yzb3r/6XRh3F7z1A5sEep8EE39p32vBU/DurXb7K1+F5N4w/CKbCHK+grE/tO//6e9h/pO28E/oAddPt++3fiYce2lTLJMfav75vT6bBABOug2KNoE30jYtLfybfX+wNYqsy+1w9MzTAIHpt9vEsWYGFG60NaLBk+GcP8C0SVBbDrGpthbgi4L4bnDTx/D6NTYJJPeFH30Fi561iQXglB/b2tO2L+DSaQecBL6NSxPBUFuVNMZma2DCkHReX5LD8txiRvdN+ZYXUOowVlsB79wK6/4HmeOhci/sXmlPfm6day+sbM8nv7OFb8FGuOwFWwtY8QZ0ybRt6itet4V71uW2wPvsj/YMetA5tkB/4Tzb/BHXFSbcCzuX2UQx8hpI6mUT1Ee/gsgESO0PS/5lawTjfwpd+tq/fuNsLBf+HWb/Fs5+0MYRnWiTUd4yGHc3xKXa7VIyoddo+/4jr4bUAbD8NZuMumfZZRExdtu0uzp+HKPi4aKpTc8rCmDVO7bWMvbW5oXyyKth8xzbtAX2rH/PWpj1a9v34IuBW+bY2sKb33c+31SbyC5+Dl6/Fs75k41z3F2Q3Ad2fA3fub+pJhZC7mwaWvSc7Qy6Z439cQLFlbUc/+As7jxjEPecNfgQRKpcZ8s8eyZ69esQk2wL5Mi40LyXMbDw77Z2O/BMu2z1f+3ol8JsW1iNvBqWvw4mACf/yJ7NjroWjrkAYrrY5pqkjKbXrCiAx4bagrVgg22uGHw2vHgBnPU7W0AVbLTNPrFpUJoL/U+HCfdB35MhZxG8eiX0n2jP4FMH2HbzaZPt/7PJj8DX/4ZVb9vHkXEw/UeAwN0rbOF3oNZ/CNmz4dxHG0/uOl1the23yJwAE35mly16zg5NvewF6D/Bfm8vXgD+WtuM5Om81od9NQ25MxHkfWM7jib+X9MXBlz4zBd4BN65fdzBvb5yj+oS22nY+0SYOs6e8U1+FOK7wju32DPwbsM69lobZ0HP45vOdNuyZjp4fLawn/UbSOwFdy2Hj38NX/0duh0HA8+AQWdDv1PtWWVtua0ZvHeXvZAqWOpAGHCm3X73Sph9P9y+0G731VTb9OGLgTsWNzUVffk3+OgXMPoGOO/xbz9j3TIPXr7Utt0DfOcBOPVuqC61naOZp8E1b3bsGB2JgloegKYhqiFq5mlP2BKBiEwCngS8wD+NMQ+1WD8U+BdwPPArY8yfv+01D0kiAHj9Ovsf747FtnoGPP7xep6ek803vzmbpJiIg38PdeQozbPjxrMuh67DIHcRpA6ChG52/Zw/2TPOS6fZ5guwJxRvfB+Kt9kx6TuW2LbrxF6247RwI5xwC5z3rT9rezb9/Fl2GOMZv7Zt2SfcDBknwPv/D44533ZGPjnCnuEDdB0Oe1bbAnnpCzDmJpj8sG2rb4sxULbLdkZWl9iz+02fwtb5UO8MW8wYCzfPsvG/dJHtbL10Ghx7SfPXyV9vayMdPfsuz4f8tRARBxmjm5bvWGqPV0L3jr2OOmBhSQQi4gU2AGcBucBi4CpjzJqgbboCfYELgb2dmgiKt9txyYPPhstfBGDx1iIum/olf7liJBeO6nXw76GODHnLbJNG2U4QD0QnQ5VzlWnGWOh1vD07Fo89Q570J3sV6ke/tG3hg75jC+LeJ9rmmPecdujkPlBVbJsFPv29XZ820DYhZF0J5bvgg3vhzN/aDtjV79pCvLrY7t8lE8beYt8noScc811Y/Byc9xgUbYHT74N/TLAJp6GjsaEtfH/UVdskt/Y9GHWNrT2AjX3bAhgyOXzNLeqQCVciOBm43xhzjvP8FwDGmD+1se39QHmnJgKwY5zn/MGOKOh/OoGA4TtPzCUmwsv/7jwV0R//kaWiwJ7ppg5ovc5fZzvy4rvC8Tc0tc2ufc92rMamwsXP2jHbZTvtGPPCbDuCpmgzDJ5kz9TfuM4+B9ukcvFztiln25f2bD4yzrazJ/e27dUvnGe3jUu3TSH+Gvs8JsW249eU2kK8It8ONxx3t61pRMbDq1fYbVP6N73nMd+DK15q+lxL/w3v/Riues0W2Eq1I1zDR3sBOUHPc4ETQ/h++++UH9v/6DN/Drd9gccbwZTxA/j52yuYt7GA8YPTwx3h0clfD/+aZK/KTB9qC+DEnnbdho9sbW3QWdClX9M+NeW2LbqtM15/nb34aMOHdtz6TR/bs/jqUttRV1Nmz5pzvrLbr/6v/e6zZ9sz/V6j4cpXbDNQ31Oav/ap99hmm56jbBv5HUtgy+d2pMywC5sSSt+Tm/a59m2bWFIH2FErtRVww/sQlWBjqdprO0nrquC0x+Gdm+1+x3/f7tOQyIZdYK/oveI/djTPhg/tkMZgx18PfU6GdB3goA5cKGsElwHnGGNudp5fB4w1xtzZxrb3s48agYjcCtwK0KdPn9Hbth3C6SDWf2CbBcbcBOc9Rq3fMP6ROfRLi+W1W0/+9v3dpKFtOG3w/o12KNgIa2fYESY9j7ejRt6+CYaeD5vn2hEll/0b1r9vCzwAxLZ3d8+CD++1o0+iEu1olBNutkkhf4M9W175Jvx3Cpx4mz3D90XaMfPzHrfJJjbVFrrnP2HPwD/5XVPzywm32OGJB9Kk0hE1ZfbiqJZDNhumUPB4bAftrpV2yGFwLbSu2rbnpw2yfRgbZ9mCX2uq6gBo09C3mX0/zH8CTv8lTPg5/5y/hd+/v5Z3bj+F451J6Y4KNWW2yaEjBUllkW3r7jHCjvEGO4b6rRvtsvMeh4w2flN5y+wQxnF32Ssh/XV2hNbuVXb9kHNt4WwCcNuX9kKZ/1zS1GRy7CV2OOInD9hx8OKxbe1ZV9gz+s2fwXGX207UD35mz8rz19vtbvsCtn/pXLEZsPtd9A97lh8INCWvhjbxhB7NOy6VOoqFKxH4sJ3FZwI7sJ3FVxtjWs3iFPZEEAjYqwKXvwqjrqPirEc45dH5nJiZwrPXt3ncQqem3LYz789ZX1WxjX3k1XbUSksBP3zxF9tM0vcUOPexfTclFG6Cly+zV1UCXPK8LaCfO8Ne/AN2SOKUL2xzRWWRvXJ08T/hw/ts23diBty+wE4WNvt+e3FQ+W57Nm4CdpqAEU4b+O41dmqAiBjbIerx2gTywc/tmfzkh+3nMsa283/6oN0vdZBt8oHmr5e7xE4J0G24nj0r5Qjn8NFzgb9gh49OM8b8QUSmABhjpopId2AJkAgEgHJgmDGmtL3XDEkiAJsMPvujvVT+xCk87ruJpz7ZyKwfHc+gRH/zC286ouXY4Y4o2QHPnGjbmy+dZtuU21K11xaQ8d1tofrKZXYYYPcsuO5diHPuzbx3q72gZe0M2+4+4Aw7XM8Y+OHntsDeOs+eba//wP51G2bnSRGPHe0y50+28/LkH8G8P8O5f7YXME09zSashsQw4Ex7BeXgybbp5pXL7EiX0h126OMVzpwp2bNtE8fZfzjwcdQLnrZDEc973Hb2b19o2+DbGzaplNILyvbLB/fBV3+nYuKDTJ2znptlBom+euTm2fYMsyNqyuGVy+1cIpc8DzXOHdBiujjzppvmF+EUbrLNGB/ca6+8NMaOZb/hf7aw3bPWnhFXFtpRLV8+Y8d9e6NsJ+veLbaPY9krtj35plm20J73uC3QB5wBI6+yzSjF22DqePt+pbk2qUTG2zP8qCQba9pguPoNe4VpeT68dhXkLrYx3LPGXnq//DU7W+LoG+17LHneXlF5zZt2HpX5T9gZE4dfBCff3nZNRSnVaTQR7I/6Gnj+bDtHCvClfxjDo3aTkJiE3PKpLczBjgTZvQZ6n9C0b12VLdQ/+R1s/MguG3WtPdOOSoQfzrUTaeUsskP9zvi1LeRfucy2u+9eY7cfeh68epUdd15X0XwOdrCTZvU5yZ7x71ppC/pT77aX2b96hb26dPdKO1b9O79tGpHTYOVbtrM2ua/tKF32qq2FnHyH7ZyM7950FSlAfa2dXyalv73gqkHJDvvaxtgmoszxNkkopQ47mgj2V12VLWDju/GnBRUsmf8hb0T/Hk98N2TyQ3a0y38uth2X5/8FxtxoL/B54Xx7lg32op8t8+wEXsl9bdNMl7628B74HducEZVoO0kbzvZrK+HH39gx6MtetSNhopNswvBF2STU7Vh7pt6eWb+BL560be2X/bv9y/83zoYeWXZcvVLqqKeJ4CAEAoZfT1/F6kWf8q+0l+lSth5SBtiO1NRB9t9R18Kmz+yUt5Metu3s3Y+zo3RWvA7HXWY7Ob940o54ufhZe+ej/1xihzPeOteeSZfvgZ4jm95881w7pnx/+if89bDxYxgwMXRDIpVSRxxNBAcpEDBc9PcFlFdWM2vcOjyf/dF2sJ71O9vUs2MpRMbC5S81L8iD1dfaTtshk5tmpKwosCN+0gZ21kdRSrmUJoJDYPqyHdz12jKm3TCG0RnxJMTE4NEb2CiljhD7SgRaknXQucf1oFtiFD9/ayXH/3EuP3z5a+r8gXCHpZRSB00TQQdFeD3cNmEA/kCAc4Z3Y9aa3fzkjeUEAkdWjUoppVpy560qD9AN4zK5YZwdsTN17iYe+mAd/dPi9I5mSqkjmiaCA/TD8f3ZuLucJz/ZyIrcYhJjIvjN+cNIjY8Kd2hKKbVfNBEcIBHhDxcdS2VtPVsKKliwqZDsPeU8ddUo0uKj9A5nSqkjho4aOkTmrN/DrS8uoc5viPJ5eOWWExndNyXcYSmlFKCjhjrFxCFd+e+PxvHopVmkxUfx87dWUF3nD3dYSin1rbRp6BAa3jOJ4T2T6JYYzfXTFjH5yXnERnoZPzidiUO6MqhrPF3iIr/9hZRSqhNp01CI/HPeZuZtLKC2PsCirUX4AwaPwM/OGcqUCf31fshKqU6lVxaHWWF5DSt2lPDWklzeX7mTcQNTGZgez8BuCZzQrwtDuyeGO0Sl1FEuXDevV47U+CgmDunKhEHpDOoWz/srdrIip4SymnoAju2VyID0eAamx3PL+P5ER7QzY6hSSoWA1gjCxBjDjuIqZq/ZzfTleRSW17K9qJJjeiRyy2mZdImLZE1eKVkZSZzcP5Wl2/bSOyWWnsk6o6hSav9p09AR4tN1u/n5WyspKK9ptjwu0ktFrZ8on4cfnJrJJcf3YkB6fLv9DPllNaTFR2o/hFKqkSaCI4g/YNiUX05heS1Duicwc+VOvtlezPjBaXy6bg/Tl+UBcEK/LlxzYl9eW7yd2voAk4/twTnDu/Pyom38Y+5mhnZP4LbTB/C9ET01ISilNBEcTXaWVDFz5S6emZNNUUUtPZKiSY2PZNWO0sZtzsvqQfbuctbvLuOsYd149NIskmIiWLi5iJS4SAZ3a7820ZIxRhOJUkcBTQRHoZLKOpZuL+KUAWlER3jZXljJR6t30SM5mvOzeuIPGKbN38KjH62nb2osx/ZK4t1vdgCQHBtBZlocJZV17C6tJmBgcPcEzh7WjavG9iElLpKFmwu59217UdwZQ7tyxtBujO7bhS6xEZoYlDoCaSJwsQWbCrj1xaWU19Rzx8SB9EmJ5ZucYrYVVpAUE0GPJNv5/PX2vSzLKSYmwktKXCQ7iqvolxrLMT0SmbexgHJnhFNitI8bTunH6H4prM4robY+wMCu8Zx7bA/eW5HH/1bsJHdvFReN6slNp/bH6xEqa+vZU1pDv7Q46v0Blmzby7bCCkb16cLgbgmH7LPW+wPkl9c0fialVBNNBC63Kb+cXSXVjBuYts/tNuwu44UFW6mu9TOgazw3nNKPuCgftfUBFm8tYt2uMhZvKeLD1bta7ZsWH0lBeS19UmJJiYtkWU4xx/RI5NSBqUxflkdBeQ0/mjiQ+dkFfLO9GACfR7j+5H6MzezCkO6J9EqOYVlOMfOzC1iTV0JGl1hO6p/ChMFd+WTdbmrqApyX1YPoCC8llXU8PWcj52f1ZETvZAIBw20vL2X22j3cN2koo/okU1nr57RBaW3WYHaWVBHls0lPKTfQRKAOqXW7Sikoq2VE7yRiI338b0Uery3KYdKx3bn2pL54BKYvy+NfC7ayIreYkb2T6Z4YzQerdhEf5eM35w9jVJ9k/vH5Zt5amtv4uh6BgLH/ZqbFsbOkmspaP16P4HduANQ1IYpJx3Zn/sYCNhdUEOnz8P/OGsyeshqen7+Fod0TWLerrPE1rxrbh/smDWVvZS0LNxdSUx9gV2k1z8/bQnpCFO/cfgrLcorZXljJ0B4JjOrThfio1pfXbCmoYPHWIlJiIxnYNZ6+qbGNCaakqo6/fZZNbISPK8f2pltiNGDvdT1jeR57K2s5MTOVod0T8HhaJyV/wFDnD7S6fqS4spb1u8oYm5mizXHqoGkiUGFTXWeHvQLMWrObY3ok0jsltnF9aXUd2woqWZ1XwraiSkb2Tuak/qkkxUTgDxjmbtjD3PX5jB+cTpTPywsLtvBFdiGxkV4euiSL5+dvZuHmIgC+N6Inf7liJB+u3oVHYFlOCVPnbmozrsnHdufzDfmISGOzF4DXI2R0icHrEXweISUukt5dYpm+LI/aoFuTdomN4PQhXUmI9vHJ2j3sLKmi4WZ1GV1i6J8eT1FFTbNO/KSYCMZmpnBM9wS+3FxIdISXm0/rz6MfrSOvuJq/XjWKkb2TqakPUOcPcPVzC9mUX8HZw7pxy/j+HNMjkfgoHwFnZFn2nnLqA4beKbGM7J3c+D4lVXXU+wON98ZYnVfCM3OyERH6p8Vx+pCulFXXsaukGo8IIlBZ62drYQXHdE/k/BE9iI1s+1rTloMHquv8LM8pZnTfLvj2cQ/v9bvKqKrz0z89jsRonaI9HDQRqKNKTb0fjwgRXg+BgCFnbyUVNf42z7jnrNvDpvxy4qJ8nNAvheTYCGrrA/RMjuGL7AJ+M30V153Ul/NH9GTdzjK+3FxATlEVfmPw+w15JVWs21XGpOHdueOMgZTX1LNhVxlfbSni8w351AcM/dLieOB7w0mOiWDmqp2szitle2El1XV+bjt9ACf2T2XRlkIWbipi4ZZCthVWMrR7ArtLq9lbWUdCtI/0hCg251c0xu3zCFE+D1eN7cOLC7dRWx/A6xFGZCSRV1zNrtLqZp/zV+cew0XH9+LpT7N5bfF2auoDjMhIJj7Kx8LNhSRE+0iOjWR7UWVj7aqlSJ+H2voACdE+Lh7VC49H2FVSzVVj+7CloIKXFm4jp6iSlLhITsxModYfYMGmQoor6zgvqwe/Pm8Yn2/IZ3ivRHweDzOW7+C0QenkFFXys7dWABAd4eHu7wzmxnH9iPR6WLWjlPSEKOKjfTz28XrmbyxgR3EVUT4PsZE+0hKi+P0Fx3JcRlJjEiqpquOtpbnsLLbfU7fEaLolRtEnJZbhPZPYUlDBzpIqRvdNaXZfkOw9ZXy9rZiT+qeS0SWGnL2VzFiWhwhMGNyV2CgvyTERbd5camtBBf/331XERXk5oV8Ko/t2YXjPJCJ9TcmvIb4dxVV8vHoXCzcXEjC2FptXXMXwnkn8+MxBzfbpTJoIlDoIh3oIbUVNPXFRPgrKa3j1q+18b2RP0uKjmDZ/Cx4nAeQUVXLx8RmM6J1MQXkNK3KL+XpbMV9sKiA9PorvDOvGMd0TiYrw8OTsjby/cicJUT6q6vxcOKoXvZJjWLCpgPqAYWj3RO6dNITk2Ej2VtSyYFMhKXGR9Em1NbNAwBAV4SE9PorFW/fy8lfb+GDlLkQgPspHYUUtAGP6dmFk72R2FFfxzfZiYqO8DOth+3b+8fnmdj+vCIwbkMb1J/flzaW5zFqzm+gID8kxkewqrcbrEbrERlBUUcvEIV3pmxpHnT9ARW09X24qpLy6nvGD05m1djcD0uPZVVLF3so6YiNtU1plrb/ZezUUaR6B2EgfHoHEmAhy91a1GRs07QOQGheJ1yPU+QMEDJyYmcI3OcXU1gdIiolge1ElYJNav9Q4yqrr2VtZS219gLT4qMYk3Sclliifhz1lNXRNiGLjnnKGdk8gKyOJylo/pdX1XD22DwFjeHzWBrIykkiJjeSdb3YwbmAaPxjXjx3FVfRPiyfCKzzy0XrOO64HF47qdUC/O00ESh3Fquv83PHKN9T5A/z6/GMY2PXgR2JV1NTj89pS8r3lO0mNj+T0wentJsQ3luSwcXcZ3x3RkxW5JVTV+jl/RA9e/Wo724oq+dPFxzU2N32RXcDstbvZWVzNGcd0ZVN+OWvySrnrzEGM6df8Zk55xVVc88+v2FNazXlZPcgpqiI20ss9Zw3m2F5JAJTX1LO7tJrsPeWsyC2mb0ocGSkxLNpSRFl1PfX+AEWVdQztnsDpQ9L5anMRxVV1dImN4Ozh3YnwCl9tLiJgDPllNWTvKQeaakifrNuDV4SXbhrLoG4J7CmtZsm2vSzZupftRZUkxUSQHBtBhNfDntJqMtPi+O6InvRLi2v2WT5avYvHP95ASVUd0REe6gOmMTkNSLd9YtV1fk4blM6XmwuprQ802z8hyscvzj2Gq0/sc0DfqSYCpdQRq7rOT8CYdvstQs0fMPgD5pA36dT5A7y8cBtl1fX8cMIAquv9VNf56ZoQzfbCSpbnFpOZFsfqvBIKymu5emyfg7qfiSYCpZRyubDdqlJEJonIehHJFpH72lgvIvKUs36FiBwfyniUUkq1FrJEICJe4BlgMjAMuEpEhrXYbDIwyPm7Ffh7qOJRSinVtlDWCMYC2caYzcaYWuA14IIW21wAvGishUCyiPQIYUxKKaVaCGUi6AXkBD3PdZbt7zZKKaVCKJSJoK1xZi17pjuyDSJyq4gsEZEl+fn5hyQ4pZRSVigTQS7QO+h5BpB3ANtgjHnWGDPGGDMmPT39kAeqlFJuFspEsBgYJCKZIhIJXAnMaLHNDOB6Z/TQSUCJMWZnCGNSSinVQsiu0DDG1IvIHcBHgBeYZoxZLSJTnPVTgZnAuUA2UAncGKp4lFJKte2Iu6BMRPKBbQe4expQcAjDOZQO19g0rv1zuMYFh29sGtf+OdC4+hpj2mxbP+ISwcEQkSXtXVkXbodrbBrX/jlc44LDNzaNa/+EIq7wzIeqlFLqsKGJQCmlXM5tieDZcAewD4drbBrX/jlc44LDNzaNa/8c8rhc1UeglFKqNbfVCJRSSrWgiUAppVzONYng2+6N0Ilx9BaROSKyVkRWi8hdzvL7RWSHiCxz/s4NQ2xbRWSl8/5LnGUpIjJLRDY6/3YJQ1xDgo7LMhEpFZG7w3HMRGSaiOwRkVVBy9o9RiLyC+c3t15EzunkuB4VkXXOvT7eFZFkZ3k/EakKOm5TOzmudr+3zjpe+4jt9aC4torIMmd5pxyzfZQPof2NGWOO+j/slc2bgP5AJLAcGBamWHoAxzuPE4AN2Ps13A/8NMzHaSuQ1mLZI8B9zuP7gIcPg+9yF9A3HMcMGA8cD6z6tmPkfK/LgSgg0/kNejsxrrMBn/P44aC4+gVvF4bj1eb31pnHq73YWqx/DPhNZx6zfZQPIf2NuaVG0JF7I3QKY8xOY8zXzuMyYC2H99TbFwD/dh7/G7gwfKEAcCawyRhzoFeXHxRjzOdAUYvF7R2jC4DXjDE1xpgt2KlUxnZWXMaYj40x9c7ThdhJHTtVO8erPZ12vL4tNhER4HLg1VC9fzsxtVc+hPQ35pZEcFje90BE+gGjgK+cRXc41fhp4WiCwU4B/rGILBWRW51l3YwzEaDzb9cwxBXsSpr/5wz3MYP2j9Hh9Lv7AfBB0PNMEflGROaKyGlhiKet7+1wOl6nAbuNMRuDlnXqMWtRPoT0N+aWRNCh+x50JhGJB94G7jbGlGJv0zkAGAnsxFZLO9s4Y8zx2FuI/khExochhnaJncX2e8CbzqLD4Zjty2HxuxORXwH1wMvOop1AH2PMKOAnwCsiktiJIbX3vR0Wx8txFc1PODr1mLVRPrS7aRvL9vuYuSURdOi+B51FRCKwX/LLxph3AIwxu40xfmNMAHiOEFaJ22OMyXP+3QO868SwW5zbhzr/7unsuIJMBr42xuyGw+OYOdo7RmH/3YnI94HzgWuM06jsNCMUOo+XYtuVB3dWTPv43sJ+vABExAdcDLzesKwzj1lb5QMh/o25JRF05N4IncJpe3weWGuMeTxoefC9mi8CVrXcN8RxxYlIQsNjbEfjKuxx+r6z2feB6Z0ZVwvNztLCfcyCtHeMZgBXikiUiGQCg4BFnRWUiEwC7gW+Z4ypDFqeLiJe53F/J67NnRhXe99bWI9XkO8A64wxuQ0LOuuYtVc+EOrfWKh7wQ+XP+x9DzZgM/mvwhjHqdiq2wpgmfN3LvASsNJZPgPo0clx9ceOPlgOrG44RkAq8Amw0fk3JUzHLRYoBJKClnX6McMmop1AHfZs7KZ9HSPgV85vbj0wuZPjysa2Hzf8zqY6217ifMfLga+B73ZyXO1+b511vNqLzVn+AjClxbadcsz2UT6E9DemU0wopZTLuaVpSCmlVDs0ESillMtpIlBKKZfTRKCUUi6niUAppVxOE4FSLYiIX5rPdnrIZqt1ZrEM1/UOSrXJF+4AlDoMVRljRoY7CKU6i9YIlOogZ376h0VkkfM30FneV0Q+cSZR+0RE+jjLu4m9D8By5+8U56W8IvKcM9/8xyISE7YPpRSaCJRqS0yLpqErgtaVGmPGAk8Df3GWPQ28aIzJwk7s9pSz/ClgrjFmBHbe+9XO8kHAM8aY4UAx9qpVpcJGryxWqgURKTfGxLexfCtwhjFmszMx2C5jTKqIFGCnSahzlu80xqSJSD6QYYypCXqNfsAsY8wg5/m9QIQx5ved8NGUapPWCJTaP6adx+1t05aaoMd+tK9OhZkmAqX2zxVB/37pPF6AndEW4BpgvvP4E+A2ABHxdvKc/0p1mJ6JKNVajDg3LXd8aIxpGEIaJSJfYU+irnKW/RiYJiI/A/KBG53ldwHPishN2DP/27CzXSp1WNE+AqU6yOkjGGOMKQh3LEodSto0pJRSLqc1AqWUcjmtESillMtpIlBKKZfTRKCUUi6niUAppVxOE4FSSrnc/weiC/QAp5hlnwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imprime a evolução de erro do modelo\n",
    "plt.plot(modelo_v4.history['loss'])\n",
    "plt.plot(modelo_v4.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Treino', 'Teste'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
